{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11911059,"sourceType":"datasetVersion","datasetId":7488087},{"sourceId":11911945,"sourceType":"datasetVersion","datasetId":7488724}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:14.523794Z","iopub.execute_input":"2025-05-24T09:46:14.524342Z","iopub.status.idle":"2025-05-24T09:46:19.358029Z","shell.execute_reply.started":"2025-05-24T09:46:14.524316Z","shell.execute_reply":"2025-05-24T09:46:19.357290Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-wl5il64j\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-wl5il64j\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn \nimport numpy as np\nimport h5py\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet18, ResNet18_Weights\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import TQDMProgressBar, ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nimport argparse\nimport yaml\nimport pandas as pd\nfrom scipy import ndimage\nfrom timm.models.vision_transformer import vit_base_patch16_224\nfrom transformers import LongformerModel, LongformerConfig\nimport clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:19.359896Z","iopub.execute_input":"2025-05-24T09:46:19.360179Z","iopub.status.idle":"2025-05-24T09:46:29.321513Z","shell.execute_reply.started":"2025-05-24T09:46:19.360154Z","shell.execute_reply":"2025-05-24T09:46:29.320891Z"}},"outputs":[{"name":"stderr","text":"2025-05-24 09:46:26.616924: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748079986.640598     157 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748079986.647393     157 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"p = \"/kaggle/input/scenarios/scenarios_small_100.txt\"\nwith open(p) as file:\n    lines = [line.strip() for line in file]\nscenarios = lines\nscenarios_tokens = clip.tokenize(scenarios)\n\ndef pad_collate(batch):\n    meta, img, vego, angle, dist = zip(*batch)\n    from torch.nn.utils.rnn import pad_sequence\n    m_pad = pad_sequence(meta, batch_first=True, padding_value=0)\n    i_pad = pad_sequence(img, batch_first=True, padding_value=0)\n    vego_pad = pad_sequence(vego, batch_first=True, padding_value=0)\n    a_pad = pad_sequence(angle, batch_first=True, padding_value=0)\n    d_pad = pad_sequence(dist, batch_first=True, padding_value=0)\n    return m_pad, i_pad, vego_pad, a_pad, d_pad, None, None, None, None, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.322596Z","iopub.execute_input":"2025-05-24T09:46:29.323079Z","iopub.status.idle":"2025-05-24T09:46:29.349580Z","shell.execute_reply.started":"2025-05-24T09:46:29.323060Z","shell.execute_reply":"2025-05-24T09:46:29.349017Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nclass CommaDataset(Dataset):\n    def __init__(\n        self,\n        dataset_type=\"train\",\n        use_transform=False,\n        multitask=\"angle\",\n        ground_truth=\"desired\",\n        return_full=False, \n        dataset_path =\"/kaggle/input/filtered-chunk1\" ,\n        dataset_fraction=1.0\n    ):\n        assert dataset_type in [\"train\", \"val\", \"test\"]\n        if dataset_type == \"val\":\n            dataset_type = \"test\" \n        if dataset_type == \"test\":\n            dataset_type = \"val\" \n        \n        self.dataset_type = dataset_type\n        self.dataset_fraction = dataset_fraction\n        self.max_len = 240\n        self.ground_truth = ground_truth\n        self.multitask = multitask\n        self.use_transform = use_transform\n        self.return_full = return_full\n        self.normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        self.resize = transforms.Resize((224,224))\n        #/data1/shared/jessica/data1/data/\n        data_path = f\"{dataset_path}/filtered_chunk1_{dataset_type}.hdf5\" if ground_truth == \"regular\" else f\"{dataset_path}/filtered_chunk1_{dataset_type}.hdf5\"\n        self.people_seqs = []\n        self.h5_file = h5py.File(data_path, \"r\")\n        corrupt_idx = 62\n        self.keys = list(self.h5_file.keys())\n        if dataset_type == \"train\":\n            self.keys.pop(corrupt_idx)\n           \n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, idx):\n        person_seq = {}\n        seq_key  = self.keys[idx]\n        keys_ = self.h5_file[seq_key].keys()#'angle', 'brake', 'dist', 'gas', 'image', 'time', 'vEgo'\n        file = self.h5_file\n        \n        for key in keys_:                        \n            seq = file[seq_key][key][()]\n            seq = seq if len(seq) <= 241 else seq[1::5]\n            person_seq[key] = torch.from_numpy(np.array(seq[0:self.max_len]).astype(float)).type(torch.float32)\n        sequences = person_seq\n        distances = sequences['dist']\n        distances = ndimage.median_filter(distances, size=128, mode='nearest')\n\n        steady_state = ~np.array(sequences['gaspressed']).astype(bool) & ~np.array(sequences['brakepressed']).astype(bool) & ~np.array(sequences['leftBlinker']).astype(bool) & ~np.array(sequences['rightBlinker']).astype(bool)\n        last_idx = 0\n        desired_gap = np.zeros(distances.shape)\n\n        for i in range(len(steady_state)-1):\n            if steady_state[i] == True:\n                desired_gap[last_idx:i] = int(distances[i])\n                last_idx = i\n        desired_gap[-12:] = distances[-12:].mean().item()\n\n        distances = sequences['dist'] if self.ground_truth else desired_gap\n        images = sequences['image']\n        images = images[:,0:160, :,:]#crop the image to remove the view of the inside car console\n        images = images.permute(0,3,1,2)\n        if not self.return_full:\n            images = self.normalize(images/255.0)\n            \n        else:\n            images = images/255.0\n        images = self.resize(images)\n        images_cropped = images\n        intervention = np.array(sequences['gaspressed']).astype(bool) | np.array(sequences['brakepressed']).astype(bool) \n        res = images_cropped, images_cropped,  sequences['vEgo'],  sequences['angle'], distances\n        if self.return_full: \n            return images_cropped,  sequences['vEgo'],  sequences['angle'], distances, np.array(sequences['gaspressed']).astype(bool),  np.array(sequences['brakepressed']).astype(bool) , np.array(sequences['CruiseStateenabled']).astype(bool)\n        if self.multitask == \"distance\":\n            res = images_cropped, images_cropped, sequences['vEgo'], distances, sequences['angle']\n        if self.multitask == \"intervention\":\n            res = images_cropped, images_cropped, sequences['vEgo'], distances, torch.tensor(np.array(sequences['gaspressed']).astype(bool) | np.array(sequences['brakepressed']).astype(bool))\n        return res ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.357343Z","iopub.execute_input":"2025-05-24T09:46:29.357603Z","iopub.status.idle":"2025-05-24T09:46:29.372758Z","shell.execute_reply.started":"2025-05-24T09:46:29.357581Z","shell.execute_reply":"2025-05-24T09:46:29.372184Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def pad_to_window_size_local(input_ids: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.Tensor,\n                             one_sided_window_size: int, pad_token_id: int):\n    \n    '''A helper function to pad tokens and mask to work with the sliding_chunks implementation of Longformer self-attention.\n    Based on _pad_to_window_size from https://github.com/huggingface/transformers:\n    https://github.com/huggingface/transformers/blob/71bdc076dd4ba2f3264283d4bc8617755206dccd/src/transformers/models/longformer/modeling_longformer.py#L1516\n    Input:\n        input_ids = torch.Tensor(bsz x seqlen): ids of wordpieces\n        attention_mask = torch.Tensor(bsz x seqlen): attention mask\n        one_sided_window_size = int: window size on one side of each token\n        pad_token_id = int: tokenizer.pad_token_id\n    Returns\n        (input_ids, attention_mask) padded to length divisible by 2 * one_sided_window_size\n    '''\n    w = 2 * one_sided_window_size\n    seqlen = input_ids.size(1)\n    padding_len = (w - seqlen % w) % w\n    input_ids = F.pad(input_ids.permute(0, 2, 1), (0, padding_len), value=pad_token_id).permute(0, 2, 1)\n    attention_mask = F.pad(attention_mask, (0, padding_len), value=False)  # no attention on the padding tokens\n    position_ids = F.pad(position_ids, (1, padding_len), value=False)  # no attention on the padding tokens\n    return input_ids, attention_mask, position_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.394102Z","iopub.execute_input":"2025-05-24T09:46:29.394343Z","iopub.status.idle":"2025-05-24T09:46:29.411576Z","shell.execute_reply.started":"2025-05-24T09:46:29.394327Z","shell.execute_reply":"2025-05-24T09:46:29.411032Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n''' This is a modified version of the Longformer model from Huggingface. '''\nclass VTNLongformerModel(LongformerModel):\n    def __init__(self,\n                 embed_dim=2048,\n                 max_position_embeddings=2 * 60 * 60,\n                 num_attention_heads=16,\n                 num_hidden_layers=3,\n                 attention_mode='sliding_chunks',\n                 pad_token_id=-1,\n                 attention_window=None,\n                 intermediate_size=3072,\n                 attention_probs_dropout_prob=0.4,\n                 hidden_dropout_prob=0.5):\n\n        self.config = LongformerConfig()\n        self.config.attention_mode = attention_mode\n        self.config.intermediate_size = intermediate_size\n        self.config.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.config.hidden_dropout_prob = hidden_dropout_prob\n        self.config.attention_dilation = [1, ] * num_hidden_layers\n        self.config.attention_window = [256, ] * num_hidden_layers if attention_window is None else attention_window\n        self.config.num_hidden_layers = num_hidden_layers\n        self.config.num_attention_heads = num_attention_heads\n        self.config.pad_token_id = pad_token_id\n        self.config.max_position_embeddings = max_position_embeddings\n        self.config.hidden_size = embed_dim\n        super(VTNLongformerModel, self).__init__(self.config, add_pooling_layer=False)\n        self.embeddings.word_embeddings = None  # to avoid distributed error of unused parameters\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.412125Z","iopub.execute_input":"2025-05-24T09:46:29.412331Z","iopub.status.idle":"2025-05-24T09:46:29.431496Z","shell.execute_reply.started":"2025-05-24T09:46:29.412315Z","shell.execute_reply":"2025-05-24T09:46:29.430833Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class VTN(nn.Module):\n    def __init__(self, multitask=\"angle\", backbone=\"resnet\", device=\"cuda\", multitask_param=True, concept_features=False, train_concepts=False, return_concepts=False):\n        super(VTN, self).__init__()\n        self.device = device\n        self.return_concepts = return_concepts\n        self.train_concepts = train_concepts\n    \n        self._construct_network(multitask, backbone, multitask_param, concept_features)\n\n    def _construct_network(self, multitask, backbone, multitask_param, concept_features):\n        clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cpu\")\n        self.clip_model = clip_model\n        self.clip_preprocess = clip_preprocess\n        self.clip_model.eval()\n        self.concept_features = concept_features\n        self.backbone_name = backbone\n\n        additional_feat_size = 3 if not concept_features else len(scenarios)+3\n\n        if backbone == \"vit\":\n            print(\"using vit backbone\")\n            self.backbone = vit_base_patch16_224(pretrained=True,num_classes=0,drop_path_rate=0.0,drop_rate=0.0)\n            num_attention_heads=3 if not concept_features else 7\n            mlp_size = 768+additional_feat_size #image feature size + previous sensor feature size \n            embed_dim = 768+additional_feat_size\n        elif backbone== \"resnet\":\n            print(\"using resnet backbone\")\n            resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n            self.backbone = torch.nn.Sequential(*list(resnet.children())[:-1])\n            embed_dim = 512+additional_feat_size #image feature size + previous sensor feature size \n            num_attention_heads=5 #if not concept_features else 6\n            mlp_size = 512+additional_feat_size #image feature size + previous sensor feature size \n        elif backbone == \"none\" and concept_features:\n            print(\"using concept features\")\n            embed_dim = len(scenarios)+3\n            num_attention_heads=1\n            mlp_size = len(scenarios)+3\n        elif backbone == \"clip\":\n            print(\"using clip backbone\")\n            self.backbone = lambda x: clip_model.encode_image(x)\n            embed_dim = 512+3\n            num_attention_heads=5\n            mlp_size = 512+3\n\n        self.multitask = multitask\n        self.multitask_param = multitask_param\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n        #self.pe = PositionalEncoding(embed_dim) #Did not add positional encoding because VTN paper found better results without\n\n        self.temporal_encoder = VTNLongformerModel(\n            embed_dim=embed_dim,\n            max_position_embeddings=288,\n            num_attention_heads=num_attention_heads,\n            num_hidden_layers=3,\n            attention_mode='sliding_chunks',\n            pad_token_id=-1,\n            attention_window=[8, 8, 8],\n            intermediate_size=3072,\n            attention_probs_dropout_prob=0.3,\n            hidden_dropout_prob=0.3)\n        num_classes = 1\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(mlp_size),\n            nn.Linear(mlp_size, mlp_size),\n            nn.GELU(),\n            nn.Dropout(0.5),\n            nn.Linear(mlp_size, num_classes)\n        )\n        self.mlp_head_2 = nn.Sequential(\n                nn.LayerNorm(mlp_size),\n                nn.Linear(mlp_size, mlp_size),\n                nn.GELU(),\n                nn.Dropout(0.5),\n                nn.Linear(mlp_size, num_classes)\n            )\n        if self.multitask and self.multitask_param:\n            self.multitask_param_angle = nn.Parameter(torch.tensor([1.0]))\n            self.multitask_param_dist = nn.Parameter(torch.tensor([1.0]))\n\n    def forward(self, img, angle, distance, vego):\n        # we need to roll the previous sensor features, so that we do not include the step that we want to predict\n        # we also substitude empty 0th entry then with 1st entry\n        x = img\n        if self.concept_features:\n            s = img.shape#[batch_size, seq_len, h,w,c]\n            logits_per_image, logits_per_text = self.clip_model(img.reshape((img.shape[0]*img.shape[1], img.shape[2], img.shape[3], img.shape[4])), scenarios_tokens.to(x.device))\n            probs = logits_per_image.softmax(dim=-1)\n            probs = logits_per_image.reshape((int(img.shape[0]), int(logits_per_image.shape[0]/img.shape[0]), -1))\n            if not self.train_concepts: probs = probs.detach()\n\n        angle = torch.roll(angle, shifts=1, dims=1)\n        angle[:,0] = angle[:,1]\n        distance = torch.roll(distance, shifts=1, dims=1)\n        distance[:,0] = distance[:,1]\n        vego = torch.roll(vego, shifts=1, dims=1)\n        vego[:,0] = vego[:,1]\n\n        # spatial backbone\n        B, F, C, H, W = x.shape\n        if self.backbone_name != \"none\":\n            x = x.reshape(B * F, C, H, W)\n            x = self.backbone(x)\n            if self.backbone_name == \"clip\":\n                if not self.train_concepts:\n                    x = x.detach()\n            x = x.reshape(B, F, -1)\n            \n\n        #concatenate the sensor features \n        if self.concept_features:\n            x = torch.cat([x, probs], dim=-1) if self.backbone_name != 'none' else probs\n        x = torch.cat((x, angle.unsqueeze(-1)), dim=-1)\n        x = torch.cat((x, distance.unsqueeze(-1)), dim=-1)\n        x = torch.cat((x, vego.unsqueeze(-1)), dim=-1)\n\n        # temporal encoder (Longformer)\n        B, D, E = x.shape\n        attention_mask = torch.ones((B, D), dtype=torch.long, device=x.device)\n        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        cls_atten = torch.ones(1).expand(B, -1).to(x.device)\n        attention_mask = torch.cat((attention_mask, cls_atten), dim=1)\n        attention_mask[:, 0] = 2 # initialize the start with a special number\n\n        \n        x, attention_mask, _ = pad_to_window_size_local(\n            x,\n            attention_mask,\n            x,#position_ids, in case we wanted them\n            self.temporal_encoder.config.attention_window[0],\n            self.temporal_encoder.config.pad_token_id)\n\n        token_type_ids = torch.zeros(x.size()[:-1], dtype=torch.long, device=x.device)\n        token_type_ids[:, 0] = 1\n\n        x = self.temporal_encoder(input_ids=None,\n                                  attention_mask=attention_mask,\n                                  token_type_ids=token_type_ids,\n                                  position_ids=None,#position_ids,\n                                  inputs_embeds=x,\n                                  output_attentions=True,\n                                  output_hidden_states=None,\n                                  return_dict=True)\n        \n        # MLP head\n        attentions = x['attentions']\n        x = x[\"last_hidden_state\"]\n        \n\n        if self.multitask:\n            x2 = self.mlp_head_2(x)\n        x = self.mlp_head(x)\n        if self.multitask != \"multitask\":\n            res = x[:,1:F+1,:], attentions\n            return res, attentions # we want to exclude the starting token since we don't have any previous knowledge about it \n        else:\n           # res = (x[:,1:F+1,:], x2[:,1:F+1,:],self.multitask_param_angle, self.multitask_param_dist), attentions\n            res = (x[:,1:F+1,:], x2[:,1:F+1,:], self.multitask_param_angle, self.multitask_param_dist)\n            return res, attentions # we want to exclude the starting token since we don't have any previous knowledge about it ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.432121Z","iopub.execute_input":"2025-05-24T09:46:29.432352Z","iopub.status.idle":"2025-05-24T09:46:29.453989Z","shell.execute_reply.started":"2025-05-24T09:46:29.432336Z","shell.execute_reply":"2025-05-24T09:46:29.453485Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class LaneModule(pl.LightningModule):\n    def __init__(self, model, bs, multitask=\"angle\", dataset=\"comma\", time_horizon=1, ground_truth=\"desired\", intervention=False, dataset_path=None, dataset_fraction=1.0):\n        super(LaneModule, self).__init__()\n        self.dataset_fraction = dataset_fraction\n        self.model = model\n        self.dataset = dataset\n        self.ground_truth = ground_truth\n        self.intervention = intervention\n        self.dataset_path = dataset_path\n        self.num_workers = 4\n        self.multitask = multitask\n        self.bs = bs\n        self.time_horizon = time_horizon\n        self.loss = self.mse_loss\n        #self.save_hyperparameters(ignore=['model'])\n        self.bce_loss = nn.BCELoss()\n\n    def forward(self, x, angle, distance, vego):\n        return self.model(x, angle, distance, vego)\n\n    def mse_loss(self, input, target, mask, reduction=\"mean\"):\n        input = input.float()\n        target = target.float()\n\n        out = (input[~mask]-target[~mask])**2\n        return out.mean() if reduction == \"mean\" else out \n\n    def calculate_loss(self, logits, angle, distance):\n        sm = nn.Softmax(dim=1)\n        if self.multitask == \"multitask\":\n            logits_angle, logits_dist, param_angle, param_dist = logits\n            mask = distance.squeeze() == 0.0\n            if not self.intervention:\n                loss_angle = torch.sqrt(self.loss(logits_angle.squeeze(), angle.squeeze(), mask))\n            else: \n                angle, distance = distance, angle\n                mask = distance.squeeze() == 0.0\n                loss_angle = self.bce_loss(sm(logits_angle.float()).squeeze()[~mask], angle.float().squeeze()[~mask])\n            loss_distance = torch.sqrt(self.loss(logits_dist.squeeze(), distance.squeeze(), mask))\n            if loss_angle.isnan() or loss_distance.isnan():\n                print(\"ERROR\")\n            loss = loss_angle, loss_distance\n            self.log_dict({\"train_loss_angle\": loss_angle.detach()}, on_epoch=True, batch_size=self.bs)\n            self.log_dict({\"train_loss_distance\": loss_distance.detach()}, on_epoch=True, batch_size=self.bs)\n            return loss_angle, loss_distance, param_angle, param_dist\n        else:\n            mask = distance.squeeze() == 0.0\n            loss = torch.sqrt(self.loss(logits.squeeze(), angle.squeeze(), mask))\n            return loss\n\n    def training_step(self, batch, batch_idx):\n        _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n        logits, attns = self(image_array, angle, distance, vego)\n        loss = self.calculate_loss(logits, angle, distance)\n        if self.multitask == \"multitask\":\n            loss_angle, loss_dist, param_angle, param_dist = loss\n            param_angle, param_dist = 0.3, 0.7\n            loss = (param_angle * loss_angle) + (param_dist * loss_dist)\n            self.log_dict({\"val_loss_dist\": loss_dist}, on_epoch=True, batch_size=self.bs)\n            self.log_dict({\"val_loss_angle\": loss_angle}, on_epoch=True, batch_size=self.bs)\n        self.log_dict({\"train_loss\": loss}, on_epoch=True, batch_size=self.bs)\n        return loss\n\n    def predict_step(self, batch, batch_idx):\n        _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n        if self.time_horizon > 1:\n            logits_all = []\n            for i in range(self.time_horizon, vego.shape[1], self.time_horizon):\n                for j in range(self.time_horizon):\n                    input_ids_img, input_ids_vego, input_ids_angle, input_ids_distance = image_array[:,0:i+j, :, :, :], vego[:,0:i+j], angle[:,0:i+j], distance[:,0:i+j]\n                    if self.multitask == \"angle\" and len(logits_all) > 0:\n                        angle[:,i+j] = torch.tensor(logits_all)[-1]\n                    if self.multitask == \"distance\" and len(logits_all) > 0:\n                        distance[:,i+j] = torch.tensor(logits_all)[-1]\n                    if self.multitask == \"multitask\":\n                        logits, attns = self(input_ids_img, input_ids_angle, input_ids_distance, input_ids_vego)\n                        logits = logits[0][:, -1], logits[1][:, -1]\n                    else:\n                        logits, attns = self(input_ids_img, input_ids_angle, input_ids_distance, input_ids_vego)[:, -1]\n                    logits_all.append(logits)\n            return torch.stack(logits_all), angle[:,self.time_horizon:], distance[:,self.time_horizon:]\n\n        \n        logits, attns = self(image_array, angle, distance, vego)\n        return logits, angle, distance\n\n    def validation_step(self, batch, batch_idx):\n        _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n        logits, attns = self(image_array, angle, distance, vego)\n        loss = self.calculate_loss(logits, angle, distance)\n        if self.multitask == \"multitask\":\n            loss_angle, loss_dist, param_angle, param_dist = loss\n            param_angle, param_dist = 0.3, 0.7\n            loss = (param_angle * loss_angle) + (param_dist * loss_dist)\n            self.log_dict({\"val_loss_dist\": loss_dist}, on_epoch=True, batch_size=self.bs)\n            self.log_dict({\"val_loss_angle\": loss_angle}, on_epoch=True, batch_size=self.bs)\n        self.log_dict({\"val_loss\": loss}, on_epoch=True, batch_size=self.bs)\n        \n        return loss\n\n    def test_step(self, batch, batch_idx):\n        _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n        if self.time_horizon > 1:\n            logits_all = []\n            for i in range(self.time_horizon,vego.shape[1], self.time_horizon):\n                for j in range(self.time_horizon+ 1 ):\n                    input_ids_img, input_ids_vego, input_ids_angle, input_ids_distance = image_array[:,0:i+j, :, :, :], vego[:,0:i+j], angle[:,0:i+j], distance[:,0:i+j]\n                    if self.multitask == \"angle\":\n                        angle[:,i+j] = logits[:,-1]\n                    if self.multitask == \"distance\":\n                        distance[:,i+j] = input_ids_distance[:,-1]\n                    logits, attns = self(input_ids_img, input_ids_angle, input_ids_distance, input_ids_vego)[:, -1]\n                    logits_all.append(logits)\n            loss = self.calculate_loss(torch.stack(logits_all), angle[:,self.time_horizon:], distance[:,self.time_horizon:])\n            self.log_dict({\"test_loss\": loss}, on_epoch=True, batch_size=self.bs)\n            return loss\n    \n        _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n        logits, attns = self(image_array, angle, distance, vego)\n        loss = self.calculate_loss(logits, angle, distance)\n        \n        if self.multitask == \"multitask\":\n            loss_angle, loss_dist, param_angle, param_dist = loss\n            param_angle, param_dist = 0.3, 0.7\n            loss = (param_angle * loss_angle) + (param_dist * loss_dist)\n            self.log_dict({\"test_loss_dist\": loss_dist}, on_epoch=True, batch_size=self.bs)\n            self.log_dict({\"test_loss_angle\": loss_angle}, on_epoch=True, batch_size=self.bs)\n        self.log_dict({\"test_loss\": loss}, on_epoch=True, batch_size=self.bs)\n        return loss\n    def train_dataloader(self):\n        return self.get_dataloader(dataset_type=\"train\")\n\n    def val_dataloader(self):\n        return self.get_dataloader(dataset_type=\"val\")\n\n    def test_dataloader(self):\n        return self.get_dataloader(dataset_type=\"test\")\n\n    def predict_dataloader(self):\n        return self.get_dataloader(dataset_type=\"test\")\n\n    def configure_optimizers(self):\n        g_opt = torch.optim.Adam(self.model.parameters(), lr=1e-5, weight_decay=1e-5)\n        return g_opt\n\n    def get_dataloader(self, dataset_type):\n        if self.dataset == \"once\":\n            ds = ONCEDataset(dataset_type=dataset_type, multitask=self.multitask) \n        elif self.dataset == \"comma\":\n            ds = CommaDataset(dataset_type=dataset_type, multitask=self.multitask if not self.intervention else \"intervention\", ground_truth=self.ground_truth, dataset_path=self.dataset_path, dataset_fraction=self.dataset_fraction)\n        elif self.dataset == 'nuscenes':\n            ds = NUScenesDataset(dataset_type=dataset_type, multitask=self.multitask if not self.intervention else \"intervention\", ground_truth=self.ground_truth, max_len=20, dataset_path=self.dataset_path, dataset_fraction=self.dataset_fraction)\n        return DataLoader(ds, batch_size=self.bs, num_workers=self.num_workers, collate_fn=pad_collate)\n        \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.475168Z","iopub.execute_input":"2025-05-24T09:46:29.475385Z","iopub.status.idle":"2025-05-24T09:46:29.499438Z","shell.execute_reply.started":"2025-05-24T09:46:29.475362Z","shell.execute_reply":"2025-05-24T09:46:29.498820Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Chunk1DataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=4):\n        super().__init__()\n        self.batch_size=batch_size\n    def setup(self, stage=None):\n        self.train_ds = CommaDataset('train')\n        self.val_ds   = CommaDataset('val')\n        self.test_ds  = CommaDataset('test')\n    def train_dataloader(self): return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n    def val_dataloader(self):   return DataLoader(self.val_ds,   batch_size=self.batch_size)\n    def test_dataloader(self):  return DataLoader(self.test_ds,  batch_size=self.batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.500167Z","iopub.execute_input":"2025-05-24T09:46:29.500391Z","iopub.status.idle":"2025-05-24T09:46:29.521399Z","shell.execute_reply.started":"2025-05-24T09:46:29.500374Z","shell.execute_reply":"2025-05-24T09:46:29.520784Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def save_preds(logits, target, save_name, p):\n    b, s = target.shape\n    df = pd.DataFrame()\n    df['logits'] = logits.squeeze().reshape(b*s).tolist()\n    df['target'] = target.squeeze().reshape(b*s).tolist()\n    df.to_csv(f'{p}/{save_name}.csv', mode='a', index=False, header=False)\n\n'''Define the argument parser'''\ndef get_arg_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-task', default='angle', type=str)\n    parser.add_argument('-train', action='store_true')\n    parser.add_argument('-test', action='store_true')\n    parser.add_argument('-gpu_num', default=0, type=int)\n    parser.add_argument('-dataset', default='comma', type=str)\n    parser.add_argument('-dataset_path', default='/kaggle/input/filtered-chunk1', type=str)\n    parser.add_argument('-bs', default=8, type=int)\n    parser.add_argument('-max_epochs', default=10, type=int)\n    parser.add_argument('-ground_truth', default='desired', type=str)\n    parser.add_argument('-new_version', action='store_true')\n    parser.add_argument('-intervention_prediction', action='store_true')\n    parser.add_argument('-dev_run', action='store_true')\n    parser.add_argument('-backbone', default='resnet', type=str, choices=['resnet', 'vit'], help='Backbone model type')\n    parser.add_argument('-concept_features', action='store_true', help='Use concept features')\n    parser.add_argument('-train_concepts', action='store_true', help='Train concept features')\n    parser.add_argument('-dataset_fraction', default=1.0, type=float, help='Fraction of dataset to use')\n    return parser\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.521943Z","iopub.execute_input":"2025-05-24T09:46:29.522154Z","iopub.status.idle":"2025-05-24T09:46:29.543035Z","shell.execute_reply.started":"2025-05-24T09:46:29.522131Z","shell.execute_reply":"2025-05-24T09:46:29.542466Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    from torch.nn import DataParallel\n    # Configurazione manuale per il primo training\n    args_list = [\n        '-train',\n        '-task', 'multitask',  # o 'angle'/'distance'\n        '-dataset', 'comma',\n        '-dataset_path', '/kaggle/input/filtered-chunk1',\n        '-bs', '2',  # Batch size ridotto per iniziare\n        '-max_epochs', '1',\n        '-backbone', 'resnet',\n        '-ground_truth', 'desired',\n        '-concept_features',  # Aggiungi se necessario\n        '-train_concepts'    # Aggiungi se necessario\n    ]\n    \n    parser = get_arg_parser()\n    args = parser.parse_args(args=args_list)\n    \n    # Setup iniziale\n   # torch.cuda.empty_cache()\n   # os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:50\"\n   # if torch.cuda.device_count() > 0 and torch.cuda.get_device_capability()[0] >= 7:\n   #     torch.set_float32_matmul_precision('high')\n\n    # Inizializzazione modello\n   # device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    model = VTN(\n        multitask=args.task,\n        backbone=args.backbone,\n        concept_features=args.concept_features,\n        device=\"cpu\",\n        train_concepts=args.train_concepts\n    )\n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        device = torch.device(\"cuda\")\n        model = DataParallel(model, device_ids=[0,1])  # scegli 0,1,2… in base alle GPU\n        model.to(device)\n    else:\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n\n    module = LaneModule(model, multitask=args.task, dataset = args.dataset, bs=args.bs, ground_truth=args.ground_truth, intervention=args.intervention_prediction, dataset_path=args.dataset_path, dataset_fraction=args.dataset_fraction)\n\n\n    # Configurazione checkpoint e logger\n    ckpt_pth = f\"/kaggle/working/ckpts_final_{args.dataset}_{args.task}_{args.backbone}_{args.concept_features}_{args.dataset_fraction}\"\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=ckpt_pth,\n        filename='best-{epoch}-{val_loss:.2f}',\n        save_top_k=1,\n        monitor=\"val_loss\",\n        mode=\"min\"\n    )\n    logger = TensorBoardLogger(save_dir=ckpt_pth)\n\n    # Configurazione trainer\n    trainer = pl.Trainer(\n       accelerator=\"gpu\",\n       devices=1,                # usa 2 GPU\n       precision=\"16-mixed\",             # mixed precision FP16 per dimezzare l’uso di VRAM\n       max_epochs=args.max_epochs,\n       logger=logger,\n       callbacks=[\n            TQDMProgressBar(refresh_rate=10),\n            checkpoint_callback,\n            EarlyStopping(monitor=\"val_loss\", patience=3)\n        ],\n        enable_checkpointing=True\n    )\n\n    # Training\n    trainer.fit(module)\n    \n    # Salvataggio configurazione\n    save_path = \"/\".join(checkpoint_callback.best_model_path.split(\"/\")[:-1])\n    with open(f'{save_path}/hparams.yaml', 'w') as f:\n        yaml.dump(vars(args), f)  # Nota: usiamo vars() per convertire Namespace in dict\n    \n    print(f\"Training completato! Checkpoint salvato in: {checkpoint_callback.best_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:46:29.543709Z","iopub.execute_input":"2025-05-24T09:46:29.543947Z","iopub.status.idle":"2025-05-24T09:48:49.095596Z","shell.execute_reply.started":"2025-05-24T09:46:29.543927Z","shell.execute_reply":"2025-05-24T09:48:49.094645Z"}},"outputs":[{"name":"stdout","text":"using resnet backbone\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/ckpts_final_comma_multitask_resnet_True_1.0 exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (35) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c95839f4a054d9c903f0e061f0f1322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Training completato! Checkpoint salvato in: /kaggle/working/ckpts_final_comma_multitask_resnet_True_1.0/best-epoch=0-val_loss=32.53.ckpt\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"best_ckpt = checkpoint_callback.best_model_path\npreds = trainer.predict(module, ckpt_path=best_ckpt if best_ckpt else \"best\")\n\nfor (logits, angle_gt, dist_gt) in preds:\n    if args.task != \"multitask\":\n        # single-task\n        save_preds(\n            logits, angle_gt,\n            f\"{args.dataset}_{args.task}_{args.backbone}_{args.concept_features}_{args.n_scenarios}\",\n            save_path\n        )\n    else:\n        # estrai solo i primi due (angle_preds, dist_preds)\n        angle_preds, dist_preds = logits[0], logits[1]\n\n        save_preds(\n            angle_preds, angle_gt,\n            f\"angle_multi_{args.dataset}_{args.task}_{args.backbone}_{args.concept_features}\",\n            save_path\n        )\n        save_preds(\n            dist_preds, dist_gt,\n            f\"dist_multi_{args.dataset}_{args.task}_{args.backbone}_{args.concept_features}\",\n            save_path\n        )\n\nprint(f\"Prediction completate, CSV salvati in: {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T09:48:49.096674Z","iopub.execute_input":"2025-05-24T09:48:49.097551Z","iopub.status.idle":"2025-05-24T09:49:09.049207Z","shell.execute_reply.started":"2025-05-24T09:48:49.097526Z","shell.execute_reply":"2025-05-24T09:49:09.048402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11e0c9f2cf44a568cf5c4c42cb585f0"}},"metadata":{}},{"name":"stdout","text":"Prediction completate, CSV salvati in: /kaggle/working/ckpts_final_comma_multitask_resnet_True_1.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}