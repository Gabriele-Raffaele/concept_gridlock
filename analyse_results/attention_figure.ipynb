{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using concept features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import pad_collate\n",
    "from dataloader_comma import CommaDataset\n",
    "from dataloader_nuscenes import NUScenesDataset\n",
    "from collections import Counter\n",
    "from model import VTN\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from utils import * \n",
    "import re\n",
    "from vis_utils import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 240, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 2; 11.91 GiB total capacity; 10.65 GiB already allocated; 70.19 MiB free; 11.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb Cell 2\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepx.ucsd.edu/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(image_array\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepx.ucsd.edu/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m img, angle, distance, vego \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mgpu_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)[:, \u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m], angle\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mgpu_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)[:, \u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m], distance\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mgpu_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)[:, \u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m], vego\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mgpu_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)[:, \u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeepx.ucsd.edu/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m (logits, attns), concepts \u001b[39m=\u001b[39m model(img, angle, distance, vego)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepx.ucsd.edu/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m top5_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(concepts\u001b[39m.\u001b[39msqueeze())\u001b[39m.\u001b[39mtopk(\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mindices\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepx.ucsd.edu/home/jessica/personalized_driving_toyota/analyse_results/attention_figure.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m s \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personalized_driving_toyota/analyse_results/../model.py:160\u001b[0m, in \u001b[0;36mVTN.forward\u001b[0;34m(self, img, angle, distance, vego)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcept_features:\n\u001b[1;32m    159\u001b[0m     s \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape\u001b[39m#[batch_size, seq_len, h,w,c]\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     logits_per_image, logits_per_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclip_model(img\u001b[39m.\u001b[39;49mreshape((img\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\u001b[39m*\u001b[39;49mimg\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], img\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m], img\u001b[39m.\u001b[39;49mshape[\u001b[39m3\u001b[39;49m], img\u001b[39m.\u001b[39;49mshape[\u001b[39m4\u001b[39;49m])), scenarios_tokens\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m    161\u001b[0m     probs \u001b[39m=\u001b[39m logits_per_image\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    162\u001b[0m     probs \u001b[39m=\u001b[39m logits_per_image\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mreshape((\u001b[39mint\u001b[39m(img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), \u001b[39mint\u001b[39m(logits_per_image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mimg\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:360\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image, text):\n\u001b[1;32m    359\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_image(image)\n\u001b[0;32m--> 360\u001b[0m     text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_text(text)\n\u001b[1;32m    362\u001b[0m     \u001b[39m# normalized features\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     image_features \u001b[39m=\u001b[39m image_features \u001b[39m/\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:348\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    346\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    347\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    349\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    350\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:191\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    190\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[0;32m--> 191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/clip/model.py:162\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    161\u001b[0m     orig_type \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 162\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(x\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtype(orig_type)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 2; 11.91 GiB total capacity; 10.65 GiB already allocated; 70.19 MiB free; 11.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for j, batch in enumerate(dataloader_comma):\n",
    "    _, image_array, vego, angle, distance, m_lens, i_lens, s_lens, a_lens, d_lens = batch\n",
    "    img = image_array\n",
    "    max_len = 5\n",
    "    print(image_array.shape)\n",
    "    img, angle, distance, vego = img.to(gpu)[:, 0:10], angle.to(gpu)[:, 0:10], distance.to(gpu)[:, 0:10], vego.to(gpu)[:, 0:10]\n",
    "    (logits, attns), concepts = model(img, angle, distance, vego)\n",
    "    top5_indices = torch.tensor(concepts.squeeze()).topk(5).indices\n",
    "    s = img.shape\n",
    "    angle, distance, vego, logits, concepts = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\"), logits.detach().cpu().to(\"cpu\"), concepts.detach().cpu().to(\"cpu\")\n",
    "\n",
    "    atten = attns[0][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_0 = moving_average(speed_graph, 5)\n",
    "\n",
    "    atten = attns[1][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_1 = moving_average(speed_graph, 5)\n",
    "\n",
    "    atten = attns[2][:,:,0:concepts.shape[1]].detach()\n",
    "    seq_len = atten.shape[2]\n",
    "    alignment_array = get_aligned_attention(atten.squeeze().cpu(), seq_len)\n",
    "    speed_graph = alignment_array.sum(axis=0)[8:-8]\n",
    "    speed_graph_2 = moving_average(speed_graph, 5)\n",
    "    speed_graph = speed_graph_0 + speed_graph_1 + speed_graph_2\n",
    "    \n",
    "    f = []\n",
    "    inter = []\n",
    "    for i, elem in enumerate(top5_indices):\n",
    "        if i % 20 == 0:\n",
    "            inter = []\n",
    "        inter.extend(elem.cpu().numpy().tolist())\n",
    "        count_dict = Counter(inter)\n",
    "\n",
    "        # Get the top 5 most occurring numbers\n",
    "        top_5 = count_dict.most_common(3)\n",
    "        intermediate = []\n",
    "        for a in top_5: \n",
    "            intermediate.append(scenarios[a[0]])\n",
    "        f.append(intermediate)\n",
    "\n",
    "    \n",
    "    for i, image in enumerate(img[0]): \n",
    "        image = unorm(image).cpu()\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        # Assuming you have an image frame\n",
    "        image_frame = image.permute(1,2,0)\n",
    "\n",
    "        # Display the image frame\n",
    "        ax1.imshow(image_frame)\n",
    "        ax1.set_title('\\n'.join([h.replace(\"a photo of driving on a highway with\", \"\").replace(\"a photo of\", \"\") for h in f[i]]))\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "\n",
    "        # Remove borders\n",
    "        ax1.spines['top'].set_visible(False)\n",
    "        ax1.spines['bottom'].set_visible(False)\n",
    "        ax1.spines['left'].set_visible(False)\n",
    "        ax1.spines['right'].set_visible(False)\n",
    "\n",
    "        # Plot the speed graph\n",
    "        ax2.plot(speed_graph_1, label='Attention')\n",
    "        #ax2.plot(speed_graph_2, label='Attention Head 2')\n",
    "        #ax2.plot(speed_graph_0, label='Attention Head 3')\n",
    "        ax2.set_title(\"Attention\")\n",
    "\n",
    "        \n",
    "        ax2.plot(i, speed_graph_0[i], marker='o', markersize=10, color='r')\n",
    "        #ax2.plot(i, speed_graph_1[i], marker='o', markersize=10, color='r')\n",
    "        #ax2.plot(i, speed_graph_2[i], marker='o', markersize=10, color='r')\n",
    "        ax1.set_aspect('equal')\n",
    "        plt.legend()\n",
    "\n",
    "        # Adjust the plot layout if necessary\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "        plt.savefig(f\"/home/jessica/personalized_driving_toyota/result_images/attention/{i}.png\")\n",
    "        plt.clf()\n",
    "    image_directory = '/home/jessica/personalized_driving_toyota/result_images/attention'\n",
    "\n",
    "    # Set the output GIF file path\n",
    "    output_gif_path = f'/home/jessica/personalized_driving_toyota/attention_comma_{j}.gif'\n",
    "\n",
    "    # Set the duration (in milliseconds) for each frame in the GIF\n",
    "    frame_duration = 500\n",
    "\n",
    "    # Get a sorted list of image files in the directory\n",
    "    image_files = sorted(glob.glob(f'{image_directory}/*.png'), key=extract_number)  # Adjust the file extension if necessary\n",
    "\n",
    "    # Create a list to store the frames of the GIF\n",
    "    frames = []\n",
    "\n",
    "    # Iterate over each image file\n",
    "    for image_file in image_files:\n",
    "        # Open the image file\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        # Add the image to the list of frames\n",
    "        frames.append(image)\n",
    "\n",
    "    # Save the frames as a GIF\n",
    "    frames[0].save(output_gif_path, format='GIF', append_images=frames[1:], save_all=True,\n",
    "                duration=frame_duration, loop=0)\n",
    "    attention_weights = att\n",
    "    normalized_weights = attention_weights#np.array(attention_weights) / np.sum(attention_weights)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    heatmap = ax.pcolormesh(alignment_array[0:70, :], cmap='hot')\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Sequence Position')\n",
    "    ax.set_ylabel('Window')\n",
    "    ax.set_title('Longformer Sliding Chunk Attention')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(f\"/home/jessica/personalized_driving_toyota/attention_vis{j}.png\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
