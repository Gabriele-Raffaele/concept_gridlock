{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, RawScoresOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
    "from pytorch_grad_cam.utils.image import scale_cam_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from model import *\n",
    "from module import * \n",
    "from dataloader_comma import *\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import ttach as tta\n",
    "from typing import Callable, List, Tuple\n",
    "import imageio\n",
    "import os\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notbook to show the gradient of the model inside the rendered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    '''Since the dataloader returns normalized images, we migth need to unnormalize them again'''\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_state_dict_keys(state_dict):\n",
    "    '''function to match the state dict weights, in case they were saved with \"model.\" prefix'''\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"model.\", \"\")\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    return new_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradCAM Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers, reshape_transform):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.handles = []\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_activation))\n",
    "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
    "            # we don't use backward hook to record gradients.\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, input, output):\n",
    "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
    "            # You can only register hooks on tensor requires grad.\n",
    "            return\n",
    "\n",
    "        # Gradients are computed in reverse order\n",
    "        def _store_grad(grad):\n",
    "            if self.reshape_transform is not None:\n",
    "                grad = self.reshape_transform(grad)\n",
    "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "        output.register_hook(_store_grad)\n",
    "\n",
    "    def __call__(self, x, y, z, a):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        return self.model(x, y, z, a)\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCAM:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module],\n",
    "                 use_cuda: bool = False,\n",
    "                 reshape_transform: Callable = None,\n",
    "                 compute_input_gradient: bool = False,\n",
    "                 uses_gradients: bool = True) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.to(device=\"cuda:2\")\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.compute_input_gradient = compute_input_gradient\n",
    "        self.uses_gradients = uses_gradients\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers, reshape_transform)\n",
    "\n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor: torch.Tensor,\n",
    "                        input_tensor_dist: torch.Tensor,\n",
    "                        input_tensor_angle: torch.Tensor,\n",
    "                        input_tensor_vego: torch.Tensor,\n",
    "                        target_layers: List[torch.nn.Module],\n",
    "                        targets: List[torch.nn.Module],\n",
    "                        activations: torch.Tensor,\n",
    "                        grads: torch.Tensor) -> np.ndarray:\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor: torch.Tensor,\n",
    "                      input_tensor_dist: torch.Tensor,\n",
    "                      input_tensor_angle: torch.Tensor,\n",
    "                      input_tensor_vego: torch.Tensor,\n",
    "                      target_layer: torch.nn.Module,\n",
    "                      targets: List[torch.nn.Module],\n",
    "                      activations: torch.Tensor,\n",
    "                      grads: torch.Tensor,\n",
    "                      eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        weights = self.get_cam_weights(input_tensor,\n",
    "                                        input_tensor_dist,\n",
    "                                        input_tensor_angle,\n",
    "                                        input_tensor_vego,\n",
    "                                       target_layer,\n",
    "                                       targets,\n",
    "                                       activations,\n",
    "                                       grads)\n",
    "        weighted_activations = weights[:, :, None, None] * activations\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(weighted_activations)\n",
    "        else:\n",
    "            cam = weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                input_tensor_dist: torch.Tensor,\n",
    "                input_tensor_angle: torch.Tensor,\n",
    "                input_tensor_vego: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        if self.cuda:\n",
    "            input_tensor = input_tensor.to(device=\"cuda:2\")\n",
    "\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_dist = torch.autograd.Variable(input_tensor_dist,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_angle = torch.autograd.Variable(input_tensor_angle,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_vego = torch.autograd.Variable(input_tensor_vego,\n",
    "                                                   requires_grad=True)\n",
    "\n",
    "        outputs = self.activations_and_grads(input_tensor, input_tensor_dist, input_tensor_angle, input_tensor_vego)\n",
    "        if targets is None:\n",
    "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
    "            targets = [ClassifierOutputTarget(\n",
    "                category) for category in target_categories]\n",
    "\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            loss = sum([target(output)\n",
    "                       for target, output in zip(targets, outputs)])\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor,\n",
    "                                                   input_tensor_dist,\n",
    "                                                   input_tensor_angle,\n",
    "                                                   input_tensor_vego,\n",
    "                                                   targets,\n",
    "                                                   eigen_smooth)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "    def get_target_width_height(self,\n",
    "                                input_tensor: torch.Tensor) -> Tuple[int, int]:\n",
    "        width, height = input_tensor.size(-1), input_tensor.size(-2)\n",
    "        return width, height\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "            self,\n",
    "            input_tensor: torch.Tensor,\n",
    "            input_tensor_dist: torch.Tensor,\n",
    "            input_tensor_angle: torch.Tensor,\n",
    "            input_tensor_vego: torch.Tensor,\n",
    "            targets: List[torch.nn.Module],\n",
    "            eigen_smooth: bool) -> np.ndarray:\n",
    "        activations_list = [a.cpu().data.numpy()\n",
    "                            for a in self.activations_and_grads.activations]\n",
    "        grads_list = [g.cpu().data.numpy()\n",
    "                      for g in self.activations_and_grads.gradients]\n",
    "        target_size = self.get_target_width_height(input_tensor)\n",
    "\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "        for i in range(len(self.target_layers)):\n",
    "            target_layer = self.target_layers[i]\n",
    "            layer_activations = None\n",
    "            layer_grads = None\n",
    "            if i < len(activations_list):\n",
    "                layer_activations = activations_list[i]\n",
    "            if i < len(grads_list):\n",
    "                layer_grads = grads_list[i]\n",
    "\n",
    "            cam = self.get_cam_image(input_tensor,\n",
    "                                    input_tensor_dist,\n",
    "                                    input_tensor_angle,\n",
    "                                    input_tensor_vego,\n",
    "                                     target_layer,\n",
    "                                     targets,\n",
    "                                     layer_activations,\n",
    "                                     layer_grads,\n",
    "                                     eigen_smooth)\n",
    "            cam = np.maximum(cam, 0)\n",
    "            scaled = scale_cam_image(cam, target_size)\n",
    "            cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "        return cam_per_target_layer\n",
    "\n",
    "    def aggregate_multi_layers(\n",
    "            self,\n",
    "            cam_per_target_layer: np.ndarray) -> np.ndarray:\n",
    "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
    "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
    "        result = np.mean(cam_per_target_layer, axis=1)\n",
    "        return scale_cam_image(result)\n",
    "\n",
    "    def forward_augmentation_smoothing(self,\n",
    "                                       input_tensor: torch.Tensor,\n",
    "                                       input_tensor_dist: torch.Tensor,\n",
    "                                        input_tensor_angle: torch.Tensor,\n",
    "                                        input_tensor_vego: torch.Tensor,\n",
    "                                       targets: List[torch.nn.Module],\n",
    "                                       eigen_smooth: bool = False) -> np.ndarray:\n",
    "        transforms = tta.Compose(\n",
    "            [\n",
    "                tta.HorizontalFlip(),\n",
    "                tta.Multiply(factors=[0.9, 1, 1.1]),\n",
    "            ]\n",
    "        )\n",
    "        cams = []\n",
    "        for transform in transforms:\n",
    "            augmented_tensor = transform.augment_image(input_tensor)\n",
    "            cam = self.forward(augmented_tensor,\n",
    "                                input_tensor_dist,\n",
    "                                input_tensor_angle,\n",
    "                                input_tensor_vego,\n",
    "                               targets,\n",
    "                               eigen_smooth)\n",
    "\n",
    "            # The ttach library expects a tensor of size BxCxHxW\n",
    "            cam = cam[:, None, :, :]\n",
    "            cam = torch.from_numpy(cam)\n",
    "            cam = transform.deaugment_mask(cam)\n",
    "\n",
    "            # Back to numpy float32, HxW\n",
    "            cam = cam.numpy()\n",
    "            cam = cam[:, 0, :, :]\n",
    "            cams.append(cam)\n",
    "\n",
    "        cam = np.mean(np.float32(cams), axis=0)\n",
    "        return cam\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_tensor: torch.Tensor,\n",
    "                 input_tensor_dist: torch.Tensor,\n",
    "                input_tensor_angle: torch.Tensor,\n",
    "                input_tensor_vego: torch.Tensor,\n",
    "                 targets: List[torch.nn.Module] = None,\n",
    "                 aug_smooth: bool = False,\n",
    "                 eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        # Smooth the CAM result with test time augmentation\n",
    "        if aug_smooth is True:\n",
    "            return self.forward_augmentation_smoothing(\n",
    "                input_tensor, input_tensor_dist,\n",
    "                                input_tensor_angle,\n",
    "                                input_tensor_vego, targets, eigen_smooth)\n",
    "\n",
    "        return self.forward(input_tensor,\n",
    "                            input_tensor_dist,\n",
    "                            input_tensor_angle,\n",
    "                            input_tensor_vego,\n",
    "                            targets, eigen_smooth)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(\n",
    "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            use_cuda,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        input_tensor_dist,\n",
    "                        input_tensor_angle,\n",
    "                        input_tensor_vego,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return np.mean(grads, axis=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'distance'\n",
    "model = VTN(multitask=task, backbone=\"resnet\")\n",
    "\n",
    "#set checkpoint path\n",
    "PATH = '/home/jessica/personalized_driving_toyota/checkpoints_comma_multitask/lightning_logs/version_23/checkpoints/epoch=31-step=2464.ckpt'#'/home/jessica/personalized_driving_toyota/checkpoints_comma_distance/lightning_logs/version_37/checkpoints/epoch=39-step=1080.ckpt'\n",
    "#PATH = '/home/jessica/personalized_driving_toyota/checkpoints_comma_angle/lightning_logs/version_11/checkpoints/epoch=32-step=891.ckpt'\n",
    "\n",
    "#load model checkpoint \n",
    "checkpoint = torch.load(PATH, map_location=\"cpu\")\n",
    "checkpoint = rename_state_dict_keys(checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint)\n",
    "#move model to GPU if desired \n",
    "model = model#.to(device=\"cuda:2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CommaDataset(dataset_type=\"test\", multitask=\"distance\")\n",
    "dl = DataLoader(ds, batch_size=10, num_workers=4)\n",
    "elem = next(iter(dl))\n",
    "meta, image_array, vego, dist, angl= elem\n",
    "idx = 9 #set an index from the current batch to examine\n",
    "\n",
    "input_tensor = image_array.float()[idx].unsqueeze(0) # Create an input tensor image for your model..\n",
    "input_tensor_angle = angl.float()[idx].unsqueeze(0)\n",
    "input_tensor_dist = dist.float()[idx].unsqueeze(0)\n",
    "input_tensor_vego = vego.float()[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mn = 120\\nidx_best = None\\nfor idx, i in enumerate(dist):\\n    if i.mean() <= mn:\\n        mn = i.mean()\\n        idx_best = idx\\nidx_best, mn'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mn = 120\n",
    "idx_best = None\n",
    "for idx, i in enumerate(dist):\n",
    "    if i.mean() <= mn:\n",
    "        mn = i.mean()\n",
    "        idx_best = idx\n",
    "idx_best, mn'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.backbone[7][-1]]\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "\n",
    "targets = [RawScoresOutputTarget()]\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_cam_1 = cam(input_tensor=input_tensor,input_tensor_dist=input_tensor_dist, input_tensor_angle=input_tensor_angle,input_tensor_vego=input_tensor_vego,targets=targets)\n",
    "sample = 9\n",
    "path_save = f'./img_{task}/'\n",
    "images = []\n",
    "for i in range(grayscale_cam_1.shape[0]):\n",
    "    grayscale_cam = grayscale_cam_1[i, :]\n",
    "    im = np.array((unorm(input_tensor[0][i])).cpu().permute(1,2,0))\n",
    "    visualization = show_cam_on_image(im, grayscale_cam, use_rgb=True)\n",
    "    images.append(visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageio.mimsave(f'{path_save}/{sample}.gif', images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
