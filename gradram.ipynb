{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, RawScoresOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
    "from pytorch_grad_cam.utils.image import scale_cam_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from model import *\n",
    "from module import * \n",
    "from dataloader_comma import *\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import ttach as tta\n",
    "from typing import Callable, List, Tuple\n",
    "import imageio\n",
    "import os\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notbook to show the gradient of the model inside the rendered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    '''Since the dataloader returns normalized images, we migth need to unnormalize them again'''\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_state_dict_keys(state_dict):\n",
    "    '''function to match the state dict weights, in case they were saved with \"model.\" prefix'''\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = key.replace(\"model.\", \"\")\n",
    "        new_state_dict[new_key] = value\n",
    "\n",
    "    return new_state_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradCAM Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers, reshape_transform):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.handles = []\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_activation))\n",
    "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
    "            # we don't use backward hook to record gradients.\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, input, output):\n",
    "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
    "            # You can only register hooks on tensor requires grad.\n",
    "            return\n",
    "\n",
    "        # Gradients are computed in reverse order\n",
    "        def _store_grad(grad):\n",
    "            if self.reshape_transform is not None:\n",
    "                grad = self.reshape_transform(grad)\n",
    "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "        output.register_hook(_store_grad)\n",
    "\n",
    "    def __call__(self, x, y, z, a):\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        return self.model(x, y, z, a)\n",
    "\n",
    "    def release(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCAM:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module],\n",
    "                 use_cuda: bool = False,\n",
    "                 reshape_transform: Callable = None,\n",
    "                 compute_input_gradient: bool = False,\n",
    "                 uses_gradients: bool = True) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.to(device=\"cuda:2\")\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.compute_input_gradient = compute_input_gradient\n",
    "        self.uses_gradients = uses_gradients\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers, reshape_transform)\n",
    "\n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor: torch.Tensor,\n",
    "                        input_tensor_dist: torch.Tensor,\n",
    "                        input_tensor_angle: torch.Tensor,\n",
    "                        input_tensor_vego: torch.Tensor,\n",
    "                        target_layers: List[torch.nn.Module],\n",
    "                        targets: List[torch.nn.Module],\n",
    "                        activations: torch.Tensor,\n",
    "                        grads: torch.Tensor) -> np.ndarray:\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor: torch.Tensor,\n",
    "                      input_tensor_dist: torch.Tensor,\n",
    "                      input_tensor_angle: torch.Tensor,\n",
    "                      input_tensor_vego: torch.Tensor,\n",
    "                      target_layer: torch.nn.Module,\n",
    "                      targets: List[torch.nn.Module],\n",
    "                      activations: torch.Tensor,\n",
    "                      grads: torch.Tensor,\n",
    "                      eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        weights = self.get_cam_weights(input_tensor,\n",
    "                                        input_tensor_dist,\n",
    "                                        input_tensor_angle,\n",
    "                                        input_tensor_vego,\n",
    "                                       target_layer,\n",
    "                                       targets,\n",
    "                                       activations,\n",
    "                                       grads)\n",
    "        weighted_activations = weights[:, :, None, None] * activations\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(weighted_activations)\n",
    "        else:\n",
    "            cam = weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                input_tensor_dist: torch.Tensor,\n",
    "                input_tensor_angle: torch.Tensor,\n",
    "                input_tensor_vego: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        if self.cuda:\n",
    "            input_tensor = input_tensor.to(device=\"cuda:2\")\n",
    "\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_dist = torch.autograd.Variable(input_tensor_dist,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_angle = torch.autograd.Variable(input_tensor_angle,\n",
    "                                                   requires_grad=True)\n",
    "            input_tensor_vego = torch.autograd.Variable(input_tensor_vego,\n",
    "                                                   requires_grad=True)\n",
    "\n",
    "        outputs = self.activations_and_grads(input_tensor, input_tensor_dist, input_tensor_angle, input_tensor_vego)\n",
    "        if targets is None:\n",
    "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
    "            targets = [ClassifierOutputTarget(\n",
    "                category) for category in target_categories]\n",
    "\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            loss = sum([target(output)\n",
    "                       for target, output in zip(targets, outputs)])\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor,\n",
    "                                                   input_tensor_dist,\n",
    "                                                   input_tensor_angle,\n",
    "                                                   input_tensor_vego,\n",
    "                                                   targets,\n",
    "                                                   eigen_smooth)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "    def get_target_width_height(self,\n",
    "                                input_tensor: torch.Tensor) -> Tuple[int, int]:\n",
    "        width, height = input_tensor.size(-1), input_tensor.size(-2)\n",
    "        return width, height\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "            self,\n",
    "            input_tensor: torch.Tensor,\n",
    "            input_tensor_dist: torch.Tensor,\n",
    "            input_tensor_angle: torch.Tensor,\n",
    "            input_tensor_vego: torch.Tensor,\n",
    "            targets: List[torch.nn.Module],\n",
    "            eigen_smooth: bool) -> np.ndarray:\n",
    "        activations_list = [a.cpu().data.numpy()\n",
    "                            for a in self.activations_and_grads.activations]\n",
    "        grads_list = [g.cpu().data.numpy()\n",
    "                      for g in self.activations_and_grads.gradients]\n",
    "        target_size = self.get_target_width_height(input_tensor)\n",
    "\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "        for i in range(len(self.target_layers)):\n",
    "            target_layer = self.target_layers[i]\n",
    "            layer_activations = None\n",
    "            layer_grads = None\n",
    "            if i < len(activations_list):\n",
    "                layer_activations = activations_list[i]\n",
    "            if i < len(grads_list):\n",
    "                layer_grads = grads_list[i]\n",
    "\n",
    "            cam = self.get_cam_image(input_tensor,\n",
    "                                    input_tensor_dist,\n",
    "                                    input_tensor_angle,\n",
    "                                    input_tensor_vego,\n",
    "                                     target_layer,\n",
    "                                     targets,\n",
    "                                     layer_activations,\n",
    "                                     layer_grads,\n",
    "                                     eigen_smooth)\n",
    "            cam = np.maximum(cam, 0)\n",
    "            scaled = scale_cam_image(cam, target_size)\n",
    "            cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "        return cam_per_target_layer\n",
    "\n",
    "    def aggregate_multi_layers(\n",
    "            self,\n",
    "            cam_per_target_layer: np.ndarray) -> np.ndarray:\n",
    "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
    "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
    "        result = np.mean(cam_per_target_layer, axis=1)\n",
    "        return scale_cam_image(result)\n",
    "\n",
    "    def forward_augmentation_smoothing(self,\n",
    "                                       input_tensor: torch.Tensor,\n",
    "                                       input_tensor_dist: torch.Tensor,\n",
    "                                        input_tensor_angle: torch.Tensor,\n",
    "                                        input_tensor_vego: torch.Tensor,\n",
    "                                       targets: List[torch.nn.Module],\n",
    "                                       eigen_smooth: bool = False) -> np.ndarray:\n",
    "        transforms = tta.Compose(\n",
    "            [\n",
    "                tta.HorizontalFlip(),\n",
    "                tta.Multiply(factors=[0.9, 1, 1.1]),\n",
    "            ]\n",
    "        )\n",
    "        cams = []\n",
    "        for transform in transforms:\n",
    "            augmented_tensor = transform.augment_image(input_tensor)\n",
    "            cam = self.forward(augmented_tensor,\n",
    "                                input_tensor_dist,\n",
    "                                input_tensor_angle,\n",
    "                                input_tensor_vego,\n",
    "                               targets,\n",
    "                               eigen_smooth)\n",
    "\n",
    "            # The ttach library expects a tensor of size BxCxHxW\n",
    "            cam = cam[:, None, :, :]\n",
    "            cam = torch.from_numpy(cam)\n",
    "            cam = transform.deaugment_mask(cam)\n",
    "\n",
    "            # Back to numpy float32, HxW\n",
    "            cam = cam.numpy()\n",
    "            cam = cam[:, 0, :, :]\n",
    "            cams.append(cam)\n",
    "\n",
    "        cam = np.mean(np.float32(cams), axis=0)\n",
    "        return cam\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_tensor: torch.Tensor,\n",
    "                 input_tensor_dist: torch.Tensor,\n",
    "                input_tensor_angle: torch.Tensor,\n",
    "                input_tensor_vego: torch.Tensor,\n",
    "                 targets: List[torch.nn.Module] = None,\n",
    "                 aug_smooth: bool = False,\n",
    "                 eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        # Smooth the CAM result with test time augmentation\n",
    "        if aug_smooth is True:\n",
    "            return self.forward_augmentation_smoothing(\n",
    "                input_tensor, input_tensor_dist,\n",
    "                                input_tensor_angle,\n",
    "                                input_tensor_vego, targets, eigen_smooth)\n",
    "\n",
    "        return self.forward(input_tensor,\n",
    "                            input_tensor_dist,\n",
    "                            input_tensor_angle,\n",
    "                            input_tensor_vego,\n",
    "                            targets, eigen_smooth)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(\n",
    "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            use_cuda,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        input_tensor_dist,\n",
    "                        input_tensor_angle,\n",
    "                        input_tensor_vego,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return np.mean(grads, axis=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'distance'\n",
    "model = VTN(multitask=task, backbone=\"resnet\")\n",
    "\n",
    "#set checkpoint path\n",
    "PATH = '/home/jessica/personalized_driving_toyota/checkpoints_comma_multitask/lightning_logs/version_23/checkpoints/epoch=31-step=2464.ckpt'#'/home/jessica/personalized_driving_toyota/checkpoints_comma_distance/lightning_logs/version_37/checkpoints/epoch=39-step=1080.ckpt'\n",
    "#PATH = '/home/jessica/personalized_driving_toyota/checkpoints_comma_angle/lightning_logs/version_11/checkpoints/epoch=32-step=891.ckpt'\n",
    "\n",
    "#load model checkpoint \n",
    "checkpoint = torch.load(PATH, map_location=\"cpu\")\n",
    "checkpoint = rename_state_dict_keys(checkpoint['state_dict'])\n",
    "model.load_state_dict(checkpoint)\n",
    "#move model to GPU if desired \n",
    "model = model#.to(device=\"cuda:2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CommaDataset(dataset_type=\"test\", multitask=\"distance\")\n",
    "dl = DataLoader(ds, batch_size=10, num_workers=4)\n",
    "elem = next(iter(dl))\n",
    "meta, image_array, vego, dist, angl= elem\n",
    "idx = 9 #set an index from the current batch to examine\n",
    "\n",
    "input_tensor = image_array.float()[idx].unsqueeze(0) # Create an input tensor image for your model..\n",
    "input_tensor_angle = angl.float()[idx].unsqueeze(0)\n",
    "input_tensor_dist = dist.float()[idx].unsqueeze(0)\n",
    "input_tensor_vego = vego.float()[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mn = 120\\nidx_best = None\\nfor idx, i in enumerate(dist):\\n    if i.mean() <= mn:\\n        mn = i.mean()\\n        idx_best = idx\\nidx_best, mn'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mn = 120\n",
    "idx_best = None\n",
    "for idx, i in enumerate(dist):\n",
    "    if i.mean() <= mn:\n",
    "        mn = i.mean()\n",
    "        idx_best = idx\n",
    "idx_best, mn'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.backbone[7][-1]]\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "\n",
    "targets = [RawScoresOutputTarget()]\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_cam_1 = cam(input_tensor=input_tensor,input_tensor_dist=input_tensor_dist, input_tensor_angle=input_tensor_angle,input_tensor_vego=input_tensor_vego,targets=targets)\n",
    "sample = 9\n",
    "path_save = f'./img_{task}/'\n",
    "images = []\n",
    "for i in range(grayscale_cam_1.shape[0]):\n",
    "    grayscale_cam = grayscale_cam_1[i, :]\n",
    "    im = np.array((unorm(input_tensor[0][i])).cpu().permute(1,2,0))\n",
    "    visualization = show_cam_on_image(im, grayscale_cam, use_rgb=True)\n",
    "    images.append(visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageio.mimsave(f'{path_save}/{sample}.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jessica/personalized_driving_toyota/checkpoints_comma_distance/lightning_logs/version_40/dist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2145614624023366,\n",
       " 1.7470169067382812,\n",
       " 1.3489151000976562,\n",
       " 1.8600959777832031,\n",
       " 1.7481575012207031,\n",
       " 1.995330810546875,\n",
       " 0.3920783996582031,\n",
       " 0.8639106750488281,\n",
       " 0.9206428527832031,\n",
       " -0.7106666564941406,\n",
       " -0.6347541809082031,\n",
       " 1.5383796691894531,\n",
       " 0.46102142333983664,\n",
       " 1.8748664855957031,\n",
       " 1.743927001953118,\n",
       " 3.152507781982422,\n",
       " 2.4361534118652344,\n",
       " 2.8348121643066406,\n",
       " 2.4831886291503835,\n",
       " 2.4814300537109375,\n",
       " 1.69708251953125,\n",
       " 0.5595588684082031,\n",
       " -0.8695449829101562,\n",
       " -1.7194061279296946,\n",
       " -2.086681365966797,\n",
       " -3.922290802001953,\n",
       " -3.8114013671875,\n",
       " -3.785400390625007,\n",
       " -2.902080535888672,\n",
       " -3.644329071044922,\n",
       " -2.86993408203125,\n",
       " -3.196758270263672,\n",
       " -1.767852783203125,\n",
       " -1.3274765014648438,\n",
       " -2.0670738220214915,\n",
       " 0.7120857238769531,\n",
       " 1.4728889465332031,\n",
       " 1.5974349975585938,\n",
       " 3.3567390441894602,\n",
       " 4.163597106933594,\n",
       " 3.8561477661132812,\n",
       " 3.54345703125,\n",
       " 4.446865081787109,\n",
       " 3.705463409423828,\n",
       " 3.7344436645507812,\n",
       " 3.7513389587402344,\n",
       " 3.1592025756835938,\n",
       " 3.6751937866210938,\n",
       " 4.267055511474609,\n",
       " 0.12980270385742188,\n",
       " -0.1165161132812429,\n",
       " 1.4050025939941335,\n",
       " 0.4966926574707031,\n",
       " -4.246860504150391,\n",
       " 0.3343620300292969,\n",
       " 0.07374191284179688,\n",
       " -0.8354301452636719,\n",
       " -1.6616783142089773,\n",
       " 0.05218505859375,\n",
       " -0.9993209838867188,\n",
       " -0.3398590087890625,\n",
       " -0.21207809448243609,\n",
       " -0.4363746643066406,\n",
       " 0.015766143798828125,\n",
       " 0.7147750854492188,\n",
       " 0.36962890625,\n",
       " 0.488910675048821,\n",
       " 0.1013031005859304,\n",
       " -0.7334136962890767,\n",
       " -0.8037605285644531,\n",
       " -1.5910606384277344,\n",
       " -3.21734619140625,\n",
       " -3.1552696228027415,\n",
       " -3.3956260681152344,\n",
       " -4.334621429443359,\n",
       " -3.6864891052246094,\n",
       " -5.947666168212891,\n",
       " -6.742176055908196,\n",
       " -5.251583099365234,\n",
       " -2.2840652465820312,\n",
       " -3.6449508666992188,\n",
       " -2.2726364135742188,\n",
       " -1.7830352783203125,\n",
       " -0.4012641906738281,\n",
       " -1.1556854248046875,\n",
       " -0.775249481201179,\n",
       " -1.1367607116699219,\n",
       " 0.24860763549804688,\n",
       " -0.3517913818359375,\n",
       " -0.3598365783691406,\n",
       " -1.2097244262695312,\n",
       " 0.2830963134765625,\n",
       " -0.8713417053222656,\n",
       " 0.1120758056640554,\n",
       " 0.09166717529297586,\n",
       " 1.1987762451171875,\n",
       " 0.7978858947753906,\n",
       " 1.348236083984368,\n",
       " 1.5586662292480469,\n",
       " 2.225616455078125,\n",
       " 1.0035781860351562,\n",
       " 3.2669143676757812,\n",
       " 3.205474853515625,\n",
       " 3.634845733642564,\n",
       " 4.735092163085945,\n",
       " 5.522861480712891,\n",
       " 5.8814849853515625,\n",
       " 5.713367462158203,\n",
       " 7.0295143127441335,\n",
       " 6.6255950927734375,\n",
       " 6.657951354980476,\n",
       " 6.410633087158203,\n",
       " 6.589687347412109,\n",
       " 6.7587890625,\n",
       " 7.443775177001953,\n",
       " 6.648101806640625,\n",
       " 6.125545501708984,\n",
       " 6.431331634521477,\n",
       " 6.007854461669922,\n",
       " 5.223346710205078,\n",
       " 4.571796417236328,\n",
       " 4.511234283447266,\n",
       " 3.862354278564446,\n",
       " 4.137504577636719,\n",
       " 4.4159698486328125,\n",
       " 4.149181365966797,\n",
       " 4.935356140136719,\n",
       " 5.266490936279297,\n",
       " 5.980815887451172,\n",
       " 6.629421234130859,\n",
       " 6.632228851318359,\n",
       " 7.234405517578125,\n",
       " 6.696395874023445,\n",
       " 7.030174255371094,\n",
       " 7.27191162109375,\n",
       " 6.8123626708984375,\n",
       " 5.845977783203125,\n",
       " 5.7867279052734375,\n",
       " 5.336734771728516,\n",
       " 5.193851470947266,\n",
       " 4.699325561523445,\n",
       " 4.976100921630859,\n",
       " 5.047664642333984,\n",
       " 5.366310119628913,\n",
       " 4.894920349121087,\n",
       " 5.147674560546875,\n",
       " 4.87969970703125,\n",
       " 5.006107330322266,\n",
       " 5.958335876464837,\n",
       " 6.120395660400391,\n",
       " 6.133380889892578,\n",
       " 6.079708099365234,\n",
       " 6.827831268310547,\n",
       " 4.204597473144531,\n",
       " 5.338825225830071,\n",
       " 4.045166015625,\n",
       " 3.4708290100097656,\n",
       " 2.3483848571777344,\n",
       " 2.1575851440429688,\n",
       " 1.0793495178222727,\n",
       " 2.1484146118164062,\n",
       " 0.6468315124511648,\n",
       " 2.4394569396972656,\n",
       " 2.4470252990722656,\n",
       " 2.3929710388183665,\n",
       " 2.877178192138672,\n",
       " 2.1437759399414062,\n",
       " 3.2080230712890625,\n",
       " 2.8849868774414062,\n",
       " 3.1763839721679616,\n",
       " 3.5781326293945312,\n",
       " 5.324901580810547,\n",
       " 5.133827209472656,\n",
       " 5.875400543212891,\n",
       " 6.695789337158203,\n",
       " 7.330558776855469,\n",
       " 6.327938079833984,\n",
       " 7.002204895019531,\n",
       " 5.435153961181641,\n",
       " 4.819953918457024,\n",
       " 3.5487899780273438,\n",
       " 2.8941383361816406,\n",
       " 2.80908203125,\n",
       " 1.5547332763671875,\n",
       " 1.90087890625,\n",
       " 2.2461700439453125,\n",
       " 3.4747390747070312,\n",
       " 2.5441322326660156,\n",
       " 3.427776336669922,\n",
       " 2.9379005432128906,\n",
       " 2.289924621582024,\n",
       " 2.357799530029304,\n",
       " 1.6936607360839915,\n",
       " 1.22882080078125,\n",
       " 1.688385009765618,\n",
       " 1.9531173706054688,\n",
       " 2.041576385498047,\n",
       " 1.736328125,\n",
       " 3.407367706298828,\n",
       " 2.9003448486328125,\n",
       " 4.310283660888672,\n",
       " 4.223693847656243,\n",
       " 3.181713104248047,\n",
       " 3.1559562683105398,\n",
       " 2.7444076538085938,\n",
       " 2.072551727294922,\n",
       " 1.5331573486328125,\n",
       " 1.197540283203125,\n",
       " 1.3775901794433594,\n",
       " 0.355365753173821,\n",
       " 0.9382820129394602,\n",
       " 0.21367263793945312,\n",
       " 1.1372337341308594,\n",
       " 1.0512619018554688,\n",
       " 1.6774063110351634,\n",
       " 1.9294471740722656,\n",
       " 2.4016647338867188,\n",
       " 1.3386955261230469,\n",
       " 3.3359527587890625,\n",
       " 2.5191993713378906,\n",
       " 2.2764434814453125,\n",
       " 2.5085830688476562,\n",
       " 3.51416015625,\n",
       " 3.0897903442382812,\n",
       " 3.168468475341797,\n",
       " 3.287799835205064,\n",
       " 3.6344375610351562,\n",
       " 3.5795822143554688,\n",
       " 4.1912384033203125,\n",
       " 4.1977996826171875,\n",
       " 4.109600067138672,\n",
       " 4.696838378906257,\n",
       " 3.3182411193847656,\n",
       " 4.273975372314446,\n",
       " 4.5753135681152415,\n",
       " 4.55067062377929,\n",
       " 4.5498199462890625,\n",
       " 4.567325592041016,\n",
       " 4.984439849853516,\n",
       " -5.761398315429695,\n",
       " -4.639213562011719,\n",
       " -2.3547515869140625,\n",
       " -3.831077575683601,\n",
       " -0.9940109252929759,\n",
       " -3.0267219543457102,\n",
       " -2.4049034118652344,\n",
       " -2.7909202575683523,\n",
       " -4.539653778076172,\n",
       " -3.9163360595703196,\n",
       " -3.691120147705078,\n",
       " -5.256122589111328,\n",
       " -6.707000732421875,\n",
       " -6.354160308837891,\n",
       " -7.987819671630859,\n",
       " -8.755771636962884,\n",
       " -9.092750549316413,\n",
       " -10.33365249633789,\n",
       " -9.3136978149414,\n",
       " -7.724178314208984,\n",
       " -7.0327301025390625,\n",
       " -2.6398353576660156,\n",
       " -7.2749786376953125,\n",
       " -7.176109313964844,\n",
       " -8.166072845458984,\n",
       " -6.923126220703118,\n",
       " -10.598358154296875,\n",
       " -11.156490325927734,\n",
       " -9.970199584960938,\n",
       " -10.26890563964843,\n",
       " -9.036876678466797,\n",
       " -9.925540924072266,\n",
       " -7.678310394287109,\n",
       " -6.215770721435547,\n",
       " -6.681201934814453,\n",
       " -4.8073272705078125,\n",
       " -0.338775634765625,\n",
       " -2.1800689697265625,\n",
       " -2.00634765625,\n",
       " 1.0513267517089844,\n",
       " -0.3765411376953125,\n",
       " -0.3473854064941406,\n",
       " 0.17953872680664062,\n",
       " 0.6230926513671875,\n",
       " -0.44763565063477273,\n",
       " -0.6160125732421875,\n",
       " -2.250621795654304,\n",
       " -2.943004608154297,\n",
       " -6.9290008544921875,\n",
       " -8.539024353027344,\n",
       " -9.576438903808594,\n",
       " -8.870941162109375,\n",
       " -10.815906524658203,\n",
       " -7.426303863525391,\n",
       " -7.615406036376953,\n",
       " -5.620471954345703,\n",
       " -4.7115631103515625,\n",
       " -5.525600433349609,\n",
       " -6.526405334472656,\n",
       " -5.690044403076172,\n",
       " -5.1251373291015625,\n",
       " -4.500556945800781,\n",
       " -6.059944152832031,\n",
       " -5.622013092041016,\n",
       " -4.799152374267578,\n",
       " -5.476367950439453,\n",
       " -5.224735260009766,\n",
       " -8.313228607177734,\n",
       " -6.7293548583984375,\n",
       " -5.3253173828125,\n",
       " -6.428028106689453,\n",
       " -3.5848846435546875,\n",
       " -2.582744598388672,\n",
       " -5.237781524658203,\n",
       " -2.1144256591796804,\n",
       " -1.7746849060058523,\n",
       " -2.1262168884277344,\n",
       " -2.2669143676757812,\n",
       " -3.3329505920410156,\n",
       " -3.3450851440429688,\n",
       " -2.8503074645996094,\n",
       " -4.522239685058601,\n",
       " -3.2243614196777415,\n",
       " -6.047367095947266,\n",
       " -5.504371643066406,\n",
       " -5.6188507080078125,\n",
       " -7.785915374755859,\n",
       " -6.949913024902337,\n",
       " -6.576656341552734,\n",
       " -8.605289459228516,\n",
       " -8.306049346923821,\n",
       " -9.31808090209961,\n",
       " -7.121063232421875,\n",
       " -8.48714828491211,\n",
       " -6.046882629394531,\n",
       " -7.47831726074218,\n",
       " -2.6719932556152344,\n",
       " -3.657238006591797,\n",
       " -6.080738067626953,\n",
       " -2.0818443298339844,\n",
       " -4.028331756591797,\n",
       " -1.622894287109375,\n",
       " -2.4681167602539062,\n",
       " -2.9047508239746094,\n",
       " -2.8166236877441406,\n",
       " -2.3835182189941335,\n",
       " -2.2592353820800852,\n",
       " -3.4064254760742188,\n",
       " -2.8377647399902344,\n",
       " -2.6629714965820312,\n",
       " -3.307037353515625,\n",
       " -3.2900123596191406,\n",
       " -3.9740753173828125,\n",
       " -3.6175880432128906,\n",
       " -1.8399734497070312,\n",
       " -3.068042755126953,\n",
       " 0.03104400634765625,\n",
       " -2.113872528076172,\n",
       " 0.4109001159667969,\n",
       " -1.9375267028808594,\n",
       " -0.09736251831053977,\n",
       " -1.6887855529785156,\n",
       " -3.496234893798828,\n",
       " -4.266017913818359,\n",
       " -5.211418151855469,\n",
       " -3.9512672424316406,\n",
       " -7.208843231201172,\n",
       " -7.424232482910149,\n",
       " -7.693584442138672,\n",
       " -6.683628082275391,\n",
       " -7.857124328613281,\n",
       " -7.353878021240234,\n",
       " -5.81964111328125,\n",
       " -5.33416748046875,\n",
       " -7.238151550292976,\n",
       " -7.1711883544921875,\n",
       " -7.345081329345703,\n",
       " -7.453304290771484,\n",
       " -8.41317367553711,\n",
       " -7.871421813964844,\n",
       " -10.504985809326172,\n",
       " -9.033302307128906,\n",
       " -8.13003921508789,\n",
       " -6.817378997802734,\n",
       " -7.266750335693359,\n",
       " -5.513988494873047,\n",
       " -4.037132263183594,\n",
       " -4.186649322509766,\n",
       " -3.8520050048828054,\n",
       " -3.058074951171875,\n",
       " -1.42578125,\n",
       " -2.3983917236328125,\n",
       " -1.2415618896484375,\n",
       " -1.9226799011230469,\n",
       " -3.0661544799804688,\n",
       " -3.192535400390625,\n",
       " -3.464488983154297,\n",
       " -4.048362731933594,\n",
       " -5.549510955810547,\n",
       " -4.6416473388671875,\n",
       " -6.733001708984375,\n",
       " -5.718112945556641,\n",
       " -5.385490417480469,\n",
       " -5.497512817382805,\n",
       " -5.442180633544922,\n",
       " -5.120307922363274,\n",
       " -6.017986297607422,\n",
       " -5.037017822265625,\n",
       " -6.232051849365234,\n",
       " -5.778106689453125,\n",
       " -4.8116912841796875,\n",
       " -4.622901916503906,\n",
       " -3.8037490844726562,\n",
       " -5.501289367675781,\n",
       " -4.855323791503906,\n",
       " -4.031593322753906,\n",
       " -3.5683670043945312,\n",
       " -3.2780990600585938,\n",
       " -2.703826904296875,\n",
       " -3.5000991821289062,\n",
       " -1.9125289916992188,\n",
       " -1.1980209350585938,\n",
       " -1.7394638061523438,\n",
       " -0.23553466796875,\n",
       " -0.6737823486328125,\n",
       " -2.6007843017578125,\n",
       " -1.4356460571289062,\n",
       " 0.8385848999023438,\n",
       " 0.5514144897460938,\n",
       " 1.2886810302734375,\n",
       " 0.6407012939453125,\n",
       " 3.9454498291015625,\n",
       " 2.77215576171875,\n",
       " 3.5257644653320312,\n",
       " 2.7852554321289062,\n",
       " 7.6633758544921875,\n",
       " 2.684234619140625,\n",
       " 5.036567687988281,\n",
       " 4.3203887939453125,\n",
       " 0.8300323486328125,\n",
       " 1.12158203125,\n",
       " 2.1745834350585938,\n",
       " 1.7242813110351562,\n",
       " 1.1839599609375,\n",
       " 1.747222900390625,\n",
       " 1.794525146484375,\n",
       " 2.8313064575195312,\n",
       " 2.7335433959960938,\n",
       " 1.8929901123046875,\n",
       " 2.6332473754882812,\n",
       " 2.9012908935546875,\n",
       " 2.5518035888671875,\n",
       " 2.9626846313476562,\n",
       " 4.0507965087890625,\n",
       " 4.033607482910156,\n",
       " 4.469703674316406,\n",
       " 5.179046630859375,\n",
       " 5.482368469238281,\n",
       " 5.655265808105469,\n",
       " 5.9912567138671875,\n",
       " 6.43505859375,\n",
       " 4.71734619140625,\n",
       " 4.28717041015625,\n",
       " 5.640106201171875,\n",
       " 6.4107666015625,\n",
       " 7.057891845703125,\n",
       " 5.841926574707031,\n",
       " 6.662261962890625,\n",
       " 8.132949829101562,\n",
       " 10.220870971679688,\n",
       " 11.152824401855469,\n",
       " 12.860389709472656,\n",
       " 16.699554443359375,\n",
       " 16.238372802734375,\n",
       " 17.662582397460938,\n",
       " 18.214744567871094,\n",
       " 18.291244506835938,\n",
       " 17.98717498779297,\n",
       " -7.0924224853515625,\n",
       " -5.791507720947266,\n",
       " -4.855930328369141,\n",
       " -1.1787757873535156,\n",
       " -0.5036544799804688,\n",
       " 0.20299148559570312,\n",
       " 2.1785125732421946,\n",
       " 2.5920944213867188,\n",
       " 3.015766143798828,\n",
       " 3.742908477783203,\n",
       " 4.190605163574219,\n",
       " 2.600475311279304,\n",
       " 3.3147964477539062,\n",
       " 1.2697410583496023,\n",
       " -0.2337799072265625,\n",
       " -1.9133338928222656,\n",
       " -3.7975196838378906,\n",
       " -3.1433982849121094,\n",
       " -1.7330131530761719,\n",
       " -1.9571533203125,\n",
       " -2.586559295654297,\n",
       " -1.7208709716796875,\n",
       " -1.2925567626953196,\n",
       " -1.5554885864257812,\n",
       " -3.324970245361328,\n",
       " -3.5806655883789062,\n",
       " -3.4819679260253906,\n",
       " -3.2156448364257812,\n",
       " -3.1515655517578125,\n",
       " -1.6587409973144602,\n",
       " -1.3561248779296875,\n",
       " 0.0528106689453125,\n",
       " -0.17714309692382812,\n",
       " 0.5043220520019531,\n",
       " 0.1080322265625,\n",
       " 0.5937232971191406,\n",
       " 0.43436050415040484,\n",
       " -0.1610107421875,\n",
       " -0.9028854370117188,\n",
       " -1.322784423828125,\n",
       " -0.5788650512695312,\n",
       " -1.7611236572265625,\n",
       " -1.9467849731445312,\n",
       " -2.2729377746582102,\n",
       " -2.11285400390625,\n",
       " -3.646484375,\n",
       " -3.7105064392089844,\n",
       " -4.3118133544921875,\n",
       " -4.305503845214837,\n",
       " -5.065700531005859,\n",
       " -4.770744323730469,\n",
       " -3.7939186096191406,\n",
       " -3.6901702880859446,\n",
       " -2.4821739196777273,\n",
       " -1.4947471618652344,\n",
       " -0.84375,\n",
       " -0.3204803466796875,\n",
       " -1.7674102783203196,\n",
       " -3.062244415283203,\n",
       " -2.269207000732422,\n",
       " -4.177314758300781,\n",
       " -5.8141326904296875,\n",
       " -7.422161102294922,\n",
       " -6.864246368408203,\n",
       " -9.669189453125,\n",
       " -10.445060729980469,\n",
       " -11.04263687133789,\n",
       " -11.086414337158203,\n",
       " -12.1595458984375,\n",
       " -10.436702728271477,\n",
       " -10.356731414794922,\n",
       " -7.903530120849595,\n",
       " -8.55178451538086,\n",
       " -9.121772766113281,\n",
       " -8.188213348388679,\n",
       " -8.023975372314453,\n",
       " -7.0038909912109375,\n",
       " -8.113948822021484,\n",
       " -6.073829650878906,\n",
       " -6.054660797119141,\n",
       " -5.472564697265618,\n",
       " -6.471626281738281,\n",
       " -3.246051788330078,\n",
       " -2.9572830200195312,\n",
       " -3.4224281311035227,\n",
       " -4.357601165771484,\n",
       " -2.0926589965820312,\n",
       " -2.7278671264648438,\n",
       " -3.1261024475097656,\n",
       " -3.3205909729003977,\n",
       " -2.9460411071777344,\n",
       " -4.089599609375,\n",
       " -3.6764564514160156,\n",
       " -3.0245399475097656,\n",
       " -6.346370697021484,\n",
       " -3.553638458251953,\n",
       " -2.953887939453125,\n",
       " -2.3030662536621094,\n",
       " -2.224590301513672,\n",
       " -1.2603378295898438,\n",
       " -2.243610382080078,\n",
       " -1.02947998046875,\n",
       " -1.0668411254882812,\n",
       " -1.8688697814941406,\n",
       " -1.0122795104980469,\n",
       " -1.709442138671875,\n",
       " -3.3905487060546875,\n",
       " -1.9962921142578125,\n",
       " -1.4704627990722656,\n",
       " -3.1040191650390625,\n",
       " -3.1712913513183594,\n",
       " -0.6671142578125,\n",
       " -0.9612770080566406,\n",
       " 0.4065284729003835,\n",
       " 0.5289764404296875,\n",
       " 2.1308364868164062,\n",
       " 2.5794410705566406,\n",
       " 3.1381378173828125,\n",
       " 2.6108169555664062,\n",
       " 1.1285667419433523,\n",
       " 1.4569358825683594,\n",
       " 0.17897415161132812,\n",
       " 0.24433135986328125,\n",
       " -1.1721115112304688,\n",
       " -2.6461448669433594,\n",
       " -2.0666389465332102,\n",
       " -3.9507217407226562,\n",
       " -2.2641792297363352,\n",
       " -6.191825866699219,\n",
       " -0.7408790588378906,\n",
       " -4.622154235839844,\n",
       " 1.0609474182128906,\n",
       " 0.5628013610839915,\n",
       " 1.53955078125,\n",
       " 2.128864288330078,\n",
       " 2.5117416381835938,\n",
       " 1.9985427856445312,\n",
       " 2.081268310546875,\n",
       " 1.8539352416992188,\n",
       " 0.043926239013671875,\n",
       " 0.6601829528808523,\n",
       " 1.4681434631347656,\n",
       " 0.6734733581542969,\n",
       " 0.8081741333007812,\n",
       " 0.9596443176269531,\n",
       " 0.9930534362792969,\n",
       " 0.34939193725585227,\n",
       " -1.5820274353027344,\n",
       " -1.5828208923339844,\n",
       " -5.8049163818359375,\n",
       " -2.6153182983398438,\n",
       " -6.855804443359382,\n",
       " -4.32958984375,\n",
       " -4.639328002929695,\n",
       " -2.313884735107429,\n",
       " -3.6246871948242188,\n",
       " -1.0690345764160156,\n",
       " -1.6488075256347585,\n",
       " -1.9031448364257955,\n",
       " -1.0268478393554759,\n",
       " 0.1043853759765625,\n",
       " -0.7345008850097656,\n",
       " 1.8514671325683594,\n",
       " 0.4279060363769531,\n",
       " 1.8154449462890625,\n",
       " 2.2776107788085938,\n",
       " 2.12823486328125,\n",
       " 1.5431365966796875,\n",
       " 2.9309463500976562,\n",
       " 2.4107589721679688,\n",
       " 2.257690429687507,\n",
       " 1.822357177734375,\n",
       " 2.045276641845696,\n",
       " 2.237396240234375,\n",
       " 0.7557258605957031,\n",
       " 1.697509765625,\n",
       " 1.5006179809570312,\n",
       " 0.11838150024414062,\n",
       " -1.0286026000976491,\n",
       " -0.5782966613769531,\n",
       " 0.22173690795898438,\n",
       " -3.902843475341804,\n",
       " -1.2820930480957031,\n",
       " -1.2067947387695312,\n",
       " -0.6186294555664134,\n",
       " -1.7498130798339844,\n",
       " -0.1568145751953125,\n",
       " 0.5745162963867188,\n",
       " -0.9317474365234446,\n",
       " -0.1169891357421875,\n",
       " 1.0869369506835938,\n",
       " 0.6955032348632812,\n",
       " 1.4617538452148509,\n",
       " 0.9123077392578196,\n",
       " 0.4441871643066335,\n",
       " 1.7411842346191406,\n",
       " 2.121105194091797,\n",
       " 1.7669906616210938,\n",
       " 2.4881019592285085,\n",
       " 1.9656219482421875,\n",
       " 1.8907890319824219,\n",
       " 1.3400802612304688,\n",
       " 1.4453887939453125,\n",
       " 1.8378677368164062,\n",
       " 1.6225242614746094,\n",
       " 1.1357078552246094,\n",
       " 1.8809013366699219,\n",
       " 2.5854034423828054,\n",
       " 3.4212722778320312,\n",
       " 2.8876113891601562,\n",
       " 3.1942024230957102,\n",
       " 3.972900390625,\n",
       " 3.711071014404297,\n",
       " 3.4344482421875,\n",
       " 3.112812042236328,\n",
       " 3.203258514404297,\n",
       " 3.3320159912109375,\n",
       " 3.7310752868652344,\n",
       " 1.628265380859375,\n",
       " 2.1028671264648438,\n",
       " 1.6393089294433594,\n",
       " 2.14251708984375,\n",
       " -0.2942466735839844,\n",
       " 3.295429229736321,\n",
       " 3.396728515625,\n",
       " 4.375724792480462,\n",
       " 3.4648895263671875,\n",
       " 4.831268310546875,\n",
       " 5.242530822753906,\n",
       " 6.153984069824219,\n",
       " 5.318271636962891,\n",
       " 6.214027404785156,\n",
       " 5.415790557861335,\n",
       " 6.1469039916992045,\n",
       " 4.5097808837890625,\n",
       " 5.261798858642578,\n",
       " 3.317401885986321,\n",
       " 4.182193756103516,\n",
       " 2.7823829650878906,\n",
       " 2.5124855041503906,\n",
       " 2.564786911010742,\n",
       " 3.092998504638679,\n",
       " 3.648260116577152,\n",
       " 3.3759269714355504,\n",
       " 2.492986679077152,\n",
       " 2.2098522186279226,\n",
       " 2.739522933959961,\n",
       " 2.0511512756347656,\n",
       " 2.079591751098633,\n",
       " 2.000492095947269,\n",
       " 2.1711711883544886,\n",
       " 1.7031059265136719,\n",
       " 1.721408843994137,\n",
       " 1.3859977722167969,\n",
       " 1.1780261993408239,\n",
       " 1.3019580841064453,\n",
       " 1.396413803100586,\n",
       " 1.3038558959960902,\n",
       " 1.5916194915771449,\n",
       " 2.1054306030273438,\n",
       " 2.2581386566162074,\n",
       " 2.4503650665283203,\n",
       " 1.7813892364501989,\n",
       " 2.8201465606689418,\n",
       " 2.338388442993164,\n",
       " 2.0176849365234375,\n",
       " 2.166845321655277,\n",
       " 2.1597976684570312,\n",
       " 1.9019088745117188,\n",
       " 1.7670936584472692,\n",
       " 1.6555442810058594,\n",
       " 1.391241073608402,\n",
       " 1.0246429443359375,\n",
       " 1.212417602539066,\n",
       " 1.0318660736083984,\n",
       " 0.47418975830078125,\n",
       " 0.8326377868652379,\n",
       " 0.21309280395507812,\n",
       " -0.3227100372314453,\n",
       " -0.707178115844723,\n",
       " -0.965866088867184,\n",
       " -1.4249458312988246,\n",
       " -1.5724029541015625,\n",
       " -1.6245231628417969,\n",
       " -2.2376956939697266,\n",
       " -2.0204200744628906,\n",
       " -2.196645736694336,\n",
       " -2.1865577697753906,\n",
       " -1.826456069946289,\n",
       " -1.8690509796142614,\n",
       " -1.6067657470703125,\n",
       " -1.5251636505126918,\n",
       " -0.8452033996582031,\n",
       " -0.365880966186527,\n",
       " -0.20516777038573508,\n",
       " 0.38659667968750355,\n",
       " 0.3265342712402344,\n",
       " 0.6488361358642543,\n",
       " 0.6328220367431676,\n",
       " 0.6210918426513707,\n",
       " 0.3694801330566406,\n",
       " 0.6996631622314489,\n",
       " 0.18120193481445312,\n",
       " 11.96240234375,\n",
       " 10.402545928955082,\n",
       " 9.44605445861816,\n",
       " 8.461645126342773,\n",
       " 7.430795669555664,\n",
       " 6.307342529296875,\n",
       " 5.045820236206055,\n",
       " 4.54133415222168,\n",
       " 3.945981979370117,\n",
       " 2.923553466796875,\n",
       " 1.4006919860839844,\n",
       " 0.7572441101074219,\n",
       " 1.410064697265625,\n",
       " 0.5706863403320312,\n",
       " 1.5958061218261719,\n",
       " 0.9651641845703125,\n",
       " 1.1057090759277344,\n",
       " 2.1529159545898473,\n",
       " 1.7368831634521484,\n",
       " 2.6558990478515625,\n",
       " 2.927978515625,\n",
       " 3.5037841796875036,\n",
       " 3.5832214355468714,\n",
       " 3.394166946411133,\n",
       " 3.294027328491211,\n",
       " 3.526729583740238,\n",
       " 3.3085308074951207,\n",
       " 2.7594776153564453,\n",
       " 2.6014118194580114,\n",
       " 3.86769866943359,\n",
       " 2.9849357604980504,\n",
       " 1.6289539337158203,\n",
       " 2.0747089385986364,\n",
       " 2.2363357543945277,\n",
       " 2.3805007934570312,\n",
       " 1.2021617889404332,\n",
       " 1.4752922058105469,\n",
       " 1.7991638183593786,\n",
       " 1.9233894348144531,\n",
       " 1.9701232910156286,\n",
       " 2.2121791839599645,\n",
       " 2.3552455902099645,\n",
       " 2.026182174682617,\n",
       " 1.7765865325927734,\n",
       " 1.5372447967529226,\n",
       " 1.3345966339111328,\n",
       " 1.0952568054199219,\n",
       " 1.0377368927001953,\n",
       " 0.850173950195316,\n",
       " 0.6049823760986328,\n",
       " 0.2612934112548828,\n",
       " 0.3448772430419922,\n",
       " -0.5807056427001918,\n",
       " -0.11268424987792613,\n",
       " -0.6079311370849645,\n",
       " -0.9649906158447266,\n",
       " -1.2349643707275426,\n",
       " -1.1937789916992188,\n",
       " -1.4492244720458984,\n",
       " -1.8568096160888707,\n",
       " -1.90673828125,\n",
       " -2.520751953125,\n",
       " -1.893743515014652,\n",
       " -2.137735366821289,\n",
       " -2.1431827545166016,\n",
       " -2.38336181640625,\n",
       " -2.2971706390380895,\n",
       " -2.4353046417236293,\n",
       " -2.804571151733402,\n",
       " -3.2824726104736364,\n",
       " -3.6033401489257812,\n",
       " -3.0533733367919886,\n",
       " -3.513477325439453,\n",
       " -4.0408782958984375,\n",
       " -4.765115737915039,\n",
       " -4.677482604980462,\n",
       " -4.816318511962891,\n",
       " -4.907642364501953,\n",
       " -4.218341827392575,\n",
       " -3.2316074371337926,\n",
       " -3.1307029724121094,\n",
       " -2.5200366973876953,\n",
       " -2.2576370239257812,\n",
       " -1.4277935028076172,\n",
       " -1.2314453125,\n",
       " -1.0903091430664062,\n",
       " -0.6771163940429723,\n",
       " -0.5888385772705078,\n",
       " -0.1740036010742223,\n",
       " -0.4331398010253906,\n",
       " -0.6658229827880859,\n",
       " -0.7248420715332031,\n",
       " -0.5195369720458984,\n",
       " -0.5717735290527344,\n",
       " -0.7234783172607386,\n",
       " -0.6437549591064418,\n",
       " -0.4073047637939453,\n",
       " -1.730539321899414,\n",
       " -0.9091701507568359,\n",
       " -0.7560482025146449,\n",
       " -1.5095939636230504,\n",
       " -1.740570068359375,\n",
       " 24.066913604736328,\n",
       " 22.435962677001953,\n",
       " -6.001213073730465,\n",
       " 19.911392211914062,\n",
       " 17.12745285034179,\n",
       " 13.274652481079102,\n",
       " 12.636343002319336,\n",
       " 9.590225219726562,\n",
       " 7.111171722412109,\n",
       " 3.2811965942382812,\n",
       " 1.9503097534179616,\n",
       " 2.6298484802246094,\n",
       " 0.841480255126946,\n",
       " -0.7368354797363281,\n",
       " 0.8917770385742188,\n",
       " 2.438720703125,\n",
       " 3.1003379821777344,\n",
       " 3.657913208007809,\n",
       " 3.8588733673095703,\n",
       " 4.39527702331543,\n",
       " 5.295209884643555,\n",
       " 4.463159561157227,\n",
       " 4.892671585083004,\n",
       " 4.313013076782227,\n",
       " 3.9742870330810547,\n",
       " 4.276689529418945,\n",
       " 3.3190326690673793,\n",
       " 3.571216583251953,\n",
       " 3.80599212646484,\n",
       " 3.391555786132816,\n",
       " 4.101560592651367,\n",
       " 3.942506790161133,\n",
       " 3.8175163269043004,\n",
       " 3.432220458984375,\n",
       " 3.055099487304684,\n",
       " 4.003328323364258,\n",
       " 5.444004058837891,\n",
       " 4.985492706298828,\n",
       " 5.173295974731442,\n",
       " 4.769241333007816,\n",
       " 4.299625396728519,\n",
       " 4.215513229370117,\n",
       " 3.868782043457035,\n",
       " 3.046648025512699,\n",
       " 3.865020751953125,\n",
       " 2.5927352905273438,\n",
       " 2.60402679443359,\n",
       " 1.6456680297851562,\n",
       " 1.2709770202636683,\n",
       " 1.6493492126464773,\n",
       " 2.425683975219723,\n",
       " 1.8408489227294922,\n",
       " 1.8545551300048828,\n",
       " 1.9191226959228551,\n",
       " 1.6929759979248047,\n",
       " 1.131521224975586,\n",
       " 1.0336303710937536,\n",
       " 1.0013694763183594,\n",
       " 0.4970512390136719,\n",
       " 0.5089397430419922,\n",
       " 0.9875888824462891,\n",
       " 1.0878047943115234,\n",
       " 0.9531822204589844,\n",
       " 1.1107158660888672,\n",
       " 1.0930900573730504,\n",
       " 0.8399410247802734,\n",
       " 0.7078704833984375,\n",
       " 0.5799369812011719,\n",
       " -0.2773323059082031,\n",
       " 0.2290668487548828,\n",
       " 0.18527984619140625,\n",
       " -0.48782920837402344,\n",
       " -1.1330432891845703,\n",
       " -1.9067268371582031,\n",
       " -2.761251449584961,\n",
       " -1.602640151977539,\n",
       " 0.5324459075927699,\n",
       " 2.017950057983402,\n",
       " 2.5785217285156214,\n",
       " 4.087459564208981,\n",
       " 3.878709793090824,\n",
       " 3.53658294677734,\n",
       " 3.4193344116210938,\n",
       " 2.324460983276367,\n",
       " 3.0029640197753906,\n",
       " 1.1566123962402344,\n",
       " 1.4228286743164062,\n",
       " 15.613296508789062,\n",
       " 12.178241729736328,\n",
       " 9.971923828125,\n",
       " 9.371673583984375,\n",
       " 7.912357330322266,\n",
       " 6.48951721191407,\n",
       " 5.535003662109375,\n",
       " 6.800254821777344,\n",
       " 6.097808837890618,\n",
       " 5.440235137939453,\n",
       " 3.741008758544922,\n",
       " 5.053260803222656,\n",
       " 4.279499053955078,\n",
       " 3.3431968688964844,\n",
       " 3.700572967529304,\n",
       " 2.7036590576171875,\n",
       " -0.20400619506835938,\n",
       " -1.5783309936523438,\n",
       " -1.0849990844726562,\n",
       " -1.5675811767578196,\n",
       " -2.4167709350585938,\n",
       " -2.91729736328125,\n",
       " 0.5623855590820312,\n",
       " 2.5363540649414062,\n",
       " 3.275604248046875,\n",
       " 2.642017364501953,\n",
       " 4.701068878173828,\n",
       " 5.128471374511719,\n",
       " 5.6569366455078125,\n",
       " 4.00811767578125,\n",
       " 5.021160125732422,\n",
       " ...]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.err.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['preds', 'targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['targets'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['err'] = df['targets'] - df['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = df[df['targets'] < 10]\n",
    "six = df[df['targets'] > 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "twen = df[(df['targets'] > 10 & (df['targets'] < 20))]\n",
    "four = df[(df['targets'] > 20 & (df['targets'] < 40))]\n",
    "fift = df[(df['targets'] > 40 & (df['targets'] < 60))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = {}\n",
    "ndf[\"<10\"] = ten['err'].tolist()\n",
    "ndf[\"10-20\"] = twen['err'].tolist()\n",
    "ndf[\"20-40\"] = four['err'].tolist()\n",
    "ndf[\"40-60\"] = fift['err'].tolist()\n",
    "ndf[\">60\"] = six['err'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(1, 0, '<10'),\n",
       " Text(2, 0, '10-20'),\n",
       " Text(3, 0, '20-40'),\n",
       " Text(4, 0, '40-60'),\n",
       " Text(5, 0, '>60')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = ndf\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(my_dict.values())\n",
    "ax.set_xticklabels(my_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
